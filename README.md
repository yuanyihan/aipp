```python
# æŸ¥çœ‹å½“å‰æŒ‚è½½çš„æ•°æ®é›†ç›®å½•, è¯¥ç›®å½•ä¸‹çš„å˜æ›´é‡å¯ç¯å¢ƒåä¼šè‡ªåŠ¨è¿˜åŸ
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
!ls /home/aistudio/data
```

    data128633



```python

```

# ä½œä¸šè¯„åˆ†è¯´æ˜
- [x] 1.æ ¼å¼è§„èŒƒï¼ˆæœ‰è‡³å°‘3ä¸ªå°æ ‡é¢˜ï¼Œå†…å®¹å®Œæ•´ï¼‰ï¼Œä¸€ä¸ªå°æ ‡é¢˜5åˆ†ï¼Œæœ€é«˜20åˆ†
- [x] 2.å›¾æ–‡å¹¶èŒ‚ï¼Œä¸€å¼ å›¾5åˆ†ï¼Œæœ€é«˜20åˆ†
- [x] 3.æœ‰å¯è¿è¡Œçš„ä»£ç ï¼Œä¸”ä»£ç å†…æœ‰è¯¦ç»†æ³¨é‡Šï¼Œ20åˆ†
- [x] 4.ä»£ç å¼€æºåˆ°githubï¼Œ15åˆ†
- [x] 5.ä»£ç åŒæ­¥åˆ°giteeï¼Œ5åˆ†

# ä½œä¸šå†…å®¹

## ä¸€ã€é¡¹ç›®èƒŒæ™¯ä»‹ç»

è¯¥éƒ¨åˆ†ä¸»è¦å‘å¤§å®¶ä»‹ç»ä½ çš„é¡¹ç›®ç›®å‰ç¤¾ä¼šç ”ç©¶æƒ…å†µï¼Œç ”ç©¶çƒ­åº¦ï¼Œæˆ–è€…ç”¨ç®€çŸ­ç²¾ç‚¼çš„è¯­è¨€è®©å¤§å®¶ç†è§£ä¸ºä»€ä¹ˆè¦åšå‡ºè¿™ä¸ªé¡¹ç›®
### 1.é€‰æ‹©[æ–‡æ¡ˆè¥é”€æ•°æ®é›†](https://aistudio.baidu.com/aistudio/datasetdetail/128633)æ•°æ®é›†
è‡ªå·±çš„æ•°æ®é›† æ•°æ®æ¥æºæ˜¯æ–‡æ¡ˆè¥é”€æ•°æ®é›†ï¼ˆ5ä¸‡ç»„æ•°æ®ï¼‰
### 2.é¡¹ç›®èƒŒæ™¯/æ„ä¹‰é‡å¤§
å¯¹äºç¤¾åª’è¥é”€ç³»ç»Ÿæ¥è¯´ï¼Œæ–‡æ¡ˆçš„ç¼–å†™æ˜¯è´¹äººè´¹äº‹çš„ï¼Œè€Œç°åœ¨NLPçš„ç”Ÿæˆå¾ˆå¼ºå¤§ï¼Œæ‰€ä»¥é‡‡ç”¨ç”Ÿæˆæ¨¡å‹å‡è½»äººå·¥çš„å·¥ä½œï¼ŒèŠ‚çœäººåŠ›æˆæœ¬ã€‚


## äºŒã€æ•°æ®ä»‹ç»
æ•°æ®æ¥æºæ˜¯æ–‡æ¡ˆè¥é”€æ•°æ®é›†ï¼ˆ5ä¸‡ç»„æ•°æ®ï¼‰ã€‚
### 1.æ€»è§ˆï¼š
æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
![png](dataset.png)

### 2.æŸ¥çœ‹å’Œé¢„å¤„ç†

#### 2.2.1 linuxæŸ¥çœ‹æ•°æ®


```python
# æŸ¥çœ‹æ•°æ®
!ls /home/aistudio/data/data128633/
!ls /home/aistudio/work
!ls /home/aistudio/data/data128633/
!tree  /home/aistudio/data/data128633/
!head -21  /home/aistudio/data/data128633/æœé¥°æ•°æ®.json
```

    æœé¥°_50k.json  æœé¥°æ•°æ®.json
    æœé¥°_50k.json  æœé¥°æ•°æ®.json
    /home/aistudio/data/data128633/
    â”œâ”€â”€ æœé¥°_50k.json
    â””â”€â”€ æœé¥°æ•°æ®.json
    
    0 directories, 2 files
    {
        "1": {
            "title": "å·´æ‹‰å·´ æ‹‰ æ——ä¸‹ æ¢¦ å¤šå¤š ç«¥è£… ç”·ç«¥ æ¯›è¡« å†¬å­£ ä¸­å¤§ç«¥ æ¯›è¡« é»‘è‰²",
            "kb": {
                "é€‚ç”¨å­£èŠ‚": "å†¬å­£",
                "åšåº¦": "é€‚ä¸­",
                "é¢†å‹": "é«˜é¢†",
                "é€‚ç”¨å¹´é¾„": "9-12å²",
                "æè´¨æˆåˆ†": "é”¦çº¶",
                "å›¾æ¡ˆ": "å…¶å®ƒ",
                "ä¸Šå¸‚æ—¶é—´": "2018å†¬å­£",
                "é¢æ–™": "å…¶å®ƒ",
                "é£æ ¼": "ä¼‘é—²é£",
                "è¡£é—¨è¥Ÿ": "å¥—å¤´",
                "é€‚ç”¨æ€§åˆ«": "ç”·",
                "å®‰å…¨ç­‰çº§": "Bç±»",
                "æ¯›çº¿ç²—ç»†": "æ™®é€šæ¯›çº¿"
            },
            "ocr": "ä¸­å›½è“ï¼Œæ·±åœŸé»„ï¼Œå¥åº·å®‰å…¨ï¼ŒAé—¨è¥Ÿï¼Œé»‘è‰²ï¼Œè¡£è¢–ï¼Œé¢æ–™å±•ï¼Œäº§å“ä¿¡æ¯ï¼Œé¢†å£ï¼Œå¯æ°´æ´—ï¼Œç»†èŠ‚å±•ç¤ºï¼Œä¸å®œæš´æ™’ï¼Œä¸å¯æ¼‚ç™½ï¼ŒçŸ­æ‹‰é“¾è®¾è®¡ï¼Œç®€æ´å®ç”¨ï¼ŒåŠç‰Œä»·:239.00ï¼Œé€‚åˆå­£èŠ‚:ç§‹å†¬å­£ï¼ŒåŠå¼€é¢†è®¾è®¡ï¼Œèˆ’é€‚äº²è‚¤ï¼Œ]é¢æ–™æ„æˆï¼Œç”°å±æ€§è¯´æ˜ï¼Œä¸å¯æºè‡ªï¼Œåˆæ ¼è¯ï¼Œä¸å¯å¹²æµ",
            "reference": "ä¸‰åˆä¸€æ··çººçº±çº¿åˆ¶æˆï¼ŒæŸ”è½¯äº²è‚¤ï¼Œè´´èº«ç©¿ä¹Ÿæ²¡æœ‰æ‰æ„Ÿã€‚åŠå¼€é¢†çš„ç«‹é¢†è®¾è®¡ï¼Œåœ¨è¾ƒå‡‰çš„å¤©æ°”ï¼Œä¿æŠ¤è„–é¢ˆï¼Œç©¿è„±ä¹Ÿæ›´ä¸ºæ–¹ä¾¿ã€‚ä¾§è¢–çš„æ‹¼æ¥æ’è‰²è®¾è®¡ï¼Œå‡¸ç°ä¸ªæ€§ï¼Œå®å®ç©¿ä¸Šæ›´å¸…æ°”ã€‚"
        },


#### 2.2.2 æ•°æ®é¢„å¤„ç†å’Œåˆ†å‰²æ•°æ®é›†


```python
# åœ¨å¯è§†åŒ–ä¹‹å‰ï¼Œé¦–å…ˆå…ˆæŠŠæ•°æ®é¢„å¤„ç†ä¸€ä¸‹ï¼Œå¹¶åˆ†å‡ºæ•°æ®é›†
import json
import jieba
samples = set()
# Read json file.
json_path = '/home/aistudio/data/data128633/æœé¥°_50k.json'
with open(json_path, 'r', encoding='utf8') as file:
    jsf = json.load(file)
for jsobj in jsf.values():
    title = jsobj['title'] + ' '  # Get title.
    kb = dict(jsobj['kb']).items()  # Get attributes.
    kb_merged = ''
    for key, val in kb:
        kb_merged += key+' '+val+' '  # Merge attributes.
    ocr = ' '.join(list(jieba.cut(jsobj['ocr'])))  # Get OCR text.
    texts = []
    texts.append(title + ocr + kb_merged)  # Merge them.
    reference = ' '.join(list(jieba.cut(jsobj['reference'])))
    for text in texts:
        text=text.replace('\t',' ').replace('ã€','').replace('â€','').replace('(','').replace(')','').replace(';',' ').replace('â€œ',' ').replace('Â°',' ').replace('ï¼Œ','').replace('ã€‚\n',' ').replace(':',' ').replace('ã€‚',' ').replace('ï¼š','').replace('/',' ').replace('  ',' ').replace('  ',' ') 
        reference=reference.replace('\t',' ').replace('ã€','').replace('â€','').replace('(','').replace(')','').replace(';',' ').replace('â€œ',' ').replace('Â°',' ').replace('ï¼Œ','').replace('ã€‚\n',' ').replace(':',' ').replace('ã€‚',' ').replace('ï¼š','').replace('/',' ').replace('  ',' ').replace('  ',' ') 
        sample = text+'\t'+reference  # Seperate source and reference.
        samples.add(sample)
print(len(samples))
with open('/home/aistudio/data/data128633/æœé¥°_50k.txt', 'w', encoding='utf8') as file:
    for line in samples:
        file.write(line)
        file.write('\n')
import random
with open('/home/aistudio/work/train_50k.txt', 'w', encoding='utf8') as trainf, open('/home/aistudio/work/dev_50k.txt', 'w', encoding='utf8') as devf, open('/home/aistudio/work/test_50k.txt', 'w', encoding='utf8') as testf:
    for line in samples:
        tmp=random.random()
        if tmp<0.7:
            trainf.write(line)
            trainf.write('\n')
        elif  tmp<0.9:
            devf.write(line)
            devf.write('\n')
        else:
            testf.write(line)
            testf.write('\n')
```

    Building prefix dict from the default dictionary ...
    Dumping model to file cache /tmp/jieba.cache
    Loading model cost 0.864 seconds.
    Prefix dict has been built successfully.


    49996


#### 2.2.3 è¯äº‘å±•ç¤º


```python
!pip install wordcloud
# å®‰è£…è¯äº‘ï¼Œå¹¶æ‰“å°æ•°æ®ï¼ŒæŸ¥çœ‹ä¸€ä¸‹åˆ†å¸ƒ
```

    Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
    Collecting wordcloud
      Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1b/06/0516bdba2ebdc0d5bd476aa66f94666dd0ad6b9abda723fdf28e451db919/wordcloud-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (366 kB)
         |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366 kB 7.9 MB/s            
    [?25hRequirement already satisfied: pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from wordcloud) (8.2.0)
    Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from wordcloud) (2.2.3)
    Requirement already satisfied: numpy>=1.6.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from wordcloud) (1.19.5)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (3.0.7)
    Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.2)
    Requirement already satisfied: six>=1.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.16.0)
    Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)
    Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (2019.3)
    Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.1.0)
    Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (56.2.0)
    Installing collected packages: wordcloud
    Successfully installed wordcloud-1.8.1
    [33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.
    You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.[0m



```python
# jiebaè¿›è¡Œæ–‡æœ¬æ•°æ®çš„åˆ†è¯ #è¿™ä¸€æ­¥ä¸éœ€è¦åšï¼Œå› ä¸ºä¸Šé¢æ•°æ®å¤„ç†ä¸­å·²ç»æœ‰äº†
import jieba
from jieba import analyse
# è¯äº‘å¯è§†åŒ–
from wordcloud import WordCloud
import matplotlib.pyplot as plt  
with open('/home/aistudio/data/data128633/æœé¥°_50k.txt', mode='r', encoding='utf-8') as f:
    text = f.readlines()
def tongji(lines,index):    
    print(f'========ç¬¬{index}æ¡æ•°æ®=====')
    # print('/ '.join(lines[index].split(' ')))
    extract_tags = analyse.extract_tags(lines[index], withWeight=True)
    for i, j in extract_tags:
        pass #print(i, j)
    result = {}
    for word in extract_tags:
        result[word[0]] = word[1]
    wordcloud = WordCloud(
        background_color="white",
        max_font_size=50,
        font_path='/home/aistudio/work/simkai.ttf')
        #
    wordcloud.generate_from_frequencies(result)

    plt.figure()
    plt.axis('off')
    plt.imshow(wordcloud)
    plt.show()

# !pip install wordcloud
```


```python
tongji(text,1)
tongji(text,2)
tongji(text,3)
tongji(text,4)
tongji(text,5)
```

    ========ç¬¬1æ¡æ•°æ®=====



    
![png](output_13_1.png)
    


    ========ç¬¬2æ¡æ•°æ®=====



    
![png](output_13_3.png)
    


    ========ç¬¬3æ¡æ•°æ®=====



    
![png](output_13_5.png)
    


    ========ç¬¬4æ¡æ•°æ®=====



    
![png](output_13_7.png)
    


    ========ç¬¬5æ¡æ•°æ®=====



    
![png](output_13_9.png)
    


#### 2.2.4 ä¸ºæ¨¡å‹æ„å»ºè¯å…¸ç­‰ç»Ÿè®¡æ•°æ®


```python
#è¿™è¾¹å¤šç»Ÿè®¡ä¸€äº›æ•°æ®
# 50kçš„è¯æ±‡çš„dic
from collections import Counter
word_c={}
lenx=[]
leny=[]
word2count = Counter()
# with open('/home/aistudio/data/data128633/samæœé¥°æ•°æ®.txt', mode='r', encoding='utf-8') as f:
with open('/home/aistudio/data/data128633/æœé¥°_50k.txt', mode='r', encoding='utf-8') as f:
    texts = f.readlines()
    for line in texts:
        # print(line)
        line=line
        ss=line.split('\t')
        if len(ss)!=2:continue
        xlst,ylst=ss[0].strip().split(" ") ,ss[1].strip().split(" ")
        # print(xlst,ylst)
        lenx.append(len(xlst))
        leny.append(len(ylst))
        for i in xlst+ylst:
            if i in word_c.keys():
                word_c[i]=word_c[i]+1
            else:
                word_c[i]=1
print(max(lenx),max(leny))
print(len(word_c))
# å°†è¿™ä¸ªè¯å…¸ä¸­çš„è¯ï¼ŒæŒ‰ç…§å‡ºç°æ¬¡æ•°æ’åºï¼Œå‡ºç°æ¬¡æ•°è¶Šé«˜ï¼Œæ’åºè¶Šé å‰
# ä¸€èˆ¬æ¥è¯´ï¼Œå‡ºç°é¢‘ç‡é«˜çš„é«˜é¢‘è¯å¾€å¾€æ˜¯ï¼šIï¼Œtheï¼Œyouè¿™ç§ä»£è¯ï¼Œè€Œå‡ºç°é¢‘ç‡ä½çš„è¯ï¼Œå¾€å¾€æ˜¯ä¸€äº›åè¯ï¼Œå¦‚ï¼šnlp
word_freq_dict = sorted(word_c.items(), key = lambda x:x[1], reverse = True)
# æ„é€ 3ä¸ªä¸åŒçš„è¯å…¸ï¼Œåˆ†åˆ«å­˜å‚¨ï¼Œ
# æ¯ä¸ªè¯åˆ°idçš„æ˜ å°„å…³ç³»ï¼šword2id_dict
# æ¯ä¸ªidåˆ°è¯çš„æ˜ å°„å…³ç³»ï¼šid2word_dict
#word2id_dict.get('<start>')
word2id_dict = {'<pad>':0,'<unk>':1,'<start>':2}
id2word_dict = {0:'<pad>',1:'<unk>',2:'<start>'}
# æŒ‰ç…§é¢‘ç‡ï¼Œä»é«˜åˆ°ä½ï¼Œå¼€å§‹éå†æ¯ä¸ªå•è¯ï¼Œå¹¶ä¸ºè¿™ä¸ªå•è¯æ„é€ ä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„id
frequency=4 #å…±è®¡95755ä¸ªè¯ï¼Œè¶…è¿‡4ä¸ªé¢‘ç‡çš„è¯æ˜¯31609
for word, freq in word_freq_dict:
    if freq>frequency:
        curr_id = len(word2id_dict)
        word2id_dict[word] = curr_id
        id2word_dict[curr_id] = word
    else:
        word2id_dict[word]=1
print("è¶…è¿‡4ä¸ªé¢‘ç‡çš„è¯:",len(id2word_dict)-3)
print("å…±è®¡:",len(word2id_dict)-3)
print("å‰5ä¸ªé«˜é¢‘è¯ï¼š",word_freq_dict[:5])
# baocun
import pickle
def save_obj(obj, name ):
    with open('/home/aistudio/work/obj-'+name+'.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name ):
    with open('/home/aistudio/work/obj-' + name + '.pkl', 'rb') as f:
        return pickle.load(f)
save_obj(word_freq_dict,"word_freq_dict")
save_obj(word2id_dict,"word2id_dict")
save_obj(id2word_dict,"id2word_dict")
print(load_obj("word_freq_dict")[:6])
```

    276 56
    95755
    è¶…è¿‡4ä¸ªé¢‘ç‡çš„è¯: 31609
    å…±è®¡: 95755
    å‰5ä¸ªé«˜é¢‘è¯ï¼š [('çš„', 324132), ('è®¾è®¡', 111486), ('é¢æ–™', 107355), ('èˆ’é€‚', 86106), ('å±•ç¤º', 59808)]
    [('çš„', 324132), ('è®¾è®¡', 111486), ('é¢æ–™', 107355), ('èˆ’é€‚', 86106), ('å±•ç¤º', 59808), ('æŒ‡æ•°', 55554)]


### 3.æ•°æ®é›†ç±»çš„å®šä¹‰ï¼ˆç»§æ‰¿paddle.io.Datasetçš„ç±»ï¼‰



```python
import paddle
print(paddle.vision)
```


```python
import sys 
sys.path.append('/home/aistudio/external-libraries')
import numpy as np
import pickle
import paddle
import paddle.vision.transforms as T
print("---")
class MyYingxiaoDataset(paddle.io.Dataset):
    def __init__(self,
                 file_path='/home/aistudio/data/data128633/æœé¥°æ•°æ®_50k.txt', #æŠ½æ ·
                 x_max_len=240, # xæœ€å¤§é•¿åº¦
                 y_max_len=50, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 ):
        super(MyYingxiaoDataset, self).__init__()
        self.x =[]  # xæ•°æ®
        self.y=[] #yæ•°æ®
        self.xl =[]  # xæœ‰æ•ˆé•¿åº¦æ•°æ®
        self.yl= [] #yæœ‰æ•ˆé•¿åº¦æ•°æ®
        word2id_dict={}
        print("é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š",x_max_len,y_max_len)
        with open(word2id_dict_pkl_path, 'rb') as f:
            word2id_dict= pickle.load(f)
        with open(file_path, mode='r', encoding='utf-8') as f:
        # with open('/home/aistudio/data/data128633/æœé¥°_50k.txt', mode='r', encoding='utf-8') as f:
            texts = f.readlines()
            for line in texts:
                # print(line)
                line=line
                ss=line.split('\t')
                if len(ss)!=2:continue
                xlst,ylst=ss[0].strip().split(" ") ,ss[1].strip().split(" ")
                xid=[ word2id_dict.get(v, 0)  for v in xlst]+[0]*x_max_len
                yid=[ word2id_dict.get(v, 0)  for v in ylst]+[0]*y_max_len
                self.x.append(xid[:x_max_len])
                self.y.append(yid[:y_max_len])
                self.xl.append(min(x_max_len,len(xlst)))
                self.yl.append(min(y_max_len,len(ylst)))
        self.lenall=len(self.xl)
        print('æ ·æœ¬æ•°ç›®ï¼š',self.lenall)

    def __getitem__(self, index):
        return paddle.to_tensor(self.x[index]),paddle.to_tensor(self.xl[index]),paddle.to_tensor(self.y[index]),paddle.to_tensor(self.yl[index])

    def __len__(self):
        return self.lenall
```

    ---



```python
print(paddle.vision)
file_path='/home/aistudio/work/dev_50k.txt'
x_max_len=240 # xæœ€å¤§é•¿åº¦
y_max_len=30 # yæœ€å¤§é•¿åº¦
train_dataset = MyYingxiaoDataset( file_path=file_path,
                 x_max_len=x_max_len, # xæœ€å¤§é•¿åº¦
                 y_max_len=y_max_len, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 )

index=9
with open(file_path, mode='r', encoding='utf-8') as f:
    texts = f.readlines()[index]
xs,ys=texts.split('\t')
print("é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š",x_max_len,y_max_len)
print("åŸå§‹æŸæ¡æ•°æ®çš„XYé•¿åº¦ï¼š",len(xs.strip().split(" ")),len(ys.strip().split(" ")))
x,xl,y,yl = train_dataset[index]
print("DatasetæŸæ¡æ•°æ®çš„XYé•¿åº¦ï¼š",xl,yl)
print("DatasetæŸæ¡å¡«å……æˆ–æˆªæ–­åæ•°æ®çš„XYé•¿åº¦ï¼š",x.shape,y.shape)
print(len(train_dataset))
print(x,xl,y,yl)
for x,xl,y,yl in train_dataset:
    print(x.shape,xl.shape,y.shape,yl.shape)
    break
```

    <module 'paddle.vision' from '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/vision/__init__.py'>
    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 30
    æ ·æœ¬æ•°ç›®ï¼š 9993
    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 30
    åŸå§‹æŸæ¡æ•°æ®çš„XYé•¿åº¦ï¼š 211 29
    DatasetæŸæ¡æ•°æ®çš„XYé•¿åº¦ï¼š Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
           [211]) Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
           [29])
    DatasetæŸæ¡å¡«å……æˆ–æˆªæ–­åæ•°æ®çš„XYé•¿åº¦ï¼š [240] [30]
    9993
    Tensor(shape=[240], dtype=int64, place=CPUPlace, stop_gradient=True,
           [3263, 755 , 65  , 1446, 308 , 362 , 56  , 371 , 24  , 1380, 659 , 2935,
            35  , 3263, 113 , 2223, 127 , 906 , 829 , 1931, 5211, 869 , 2577, 2101,
            13  , 7   , 25  , 374 , 25  , 7   , 351 , 2184, 906 , 829 , 251 , 26  ,
            897 , 709 , 2977, 1225, 2730, 3225, 386 , 327 , 43  , 613 , 1877, 734 ,
            740 , 144 , 659 , 859 , 177 , 96  , 1908, 38  , 19  , 276 , 280 , 10  ,
            122 , 906 , 829 , 47  , 4   , 357 , 3   , 145 , 55  , 336 , 507 , 3   ,
            56  , 561 , 495 , 351 , 210 , 225 , 829 , 1312, 1181, 64  , 29  , 1931,
            659 , 423 , 2545, 46  , 76  , 1027, 717 , 829 , 155 , 166 , 11  , 24  ,
            3   , 113 , 280 , 96  , 109 , 285 , 1225, 6084, 2730, 3225, 5532, 27  ,
            4054, 1089, 7120, 1457, 131 , 48  , 1793, 829 , 613 , 2492, 229 , 12758,
            2003, 911 , 15  , 452 , 18117, 10746, 4   , 6819, 87  , 538 , 567 , 8504,
            210 , 225 , 1312, 10747, 14  , 63  , 27  , 332 , 633 , 56  , 755 , 2998,
            11104, 900 , 1145, 6001, 1724, 8138, 611 , 3   , 6214, 38  , 20638, 3   ,
            9739, 8822, 1297, 18854, 4   , 1454, 33  , 1   , 35  , 1312, 277 , 829 ,
            277 , 3818, 11104, 12051, 917 , 14575, 1285, 1348, 18118, 906 , 43  , 351 ,
            2231, 13558, 475 , 33  , 2088, 10145, 3   , 28634, 369 , 14229, 41  , 1   ,
            3423, 148 , 4769, 2012, 3197, 27  , 3263, 2742, 2250, 2170, 3887, 21  ,
            490 , 44  , 160 , 2284, 1   , 2261, 15349, 0   , 0   , 0   , 0   , 0   ,
            0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   ,
            0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   ]) Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
           [211]) Tensor(shape=[30], dtype=int64, place=CPUPlace, stop_gradient=True,
           [29 , 11 , 127, 906, 829, 4  , 650, 3968, 346, 131, 7121, 606, 709, 3  ,
            3115, 155, 371, 713, 492, 1446, 308, 3  , 63 , 1227, 112, 4167, 538, 159,
            6  , 0  ]) Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
           [29])
    [240] [1] [30] [1]


## å›› æ¨¡å‹è®­ç»ƒ
### 1.æ•°æ®å¤„ç†

#### 4.1.1 æ„å»ºdataset


```python
# é¦–å…ˆï¼Œæ•°æ®å¤„ç†ä¸€ä¸‹
x_max_len=240 # xæœ€å¤§é•¿åº¦
y_max_len=50 # yæœ€å¤§é•¿åº¦
train_dataset = MyYingxiaoDataset( file_path='/home/aistudio/work/train_50k.txt',
                 x_max_len=x_max_len, # xæœ€å¤§é•¿åº¦
                 y_max_len=y_max_len, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 )
val_dataset = MyYingxiaoDataset( file_path='/home/aistudio/work/dev_50k.txt',
                 x_max_len=x_max_len, # xæœ€å¤§é•¿åº¦
                 y_max_len=y_max_len, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 )
test_dataset = MyYingxiaoDataset( file_path='/home/aistudio/work/test_50k.txt',
                 x_max_len=x_max_len, # xæœ€å¤§é•¿åº¦
                 y_max_len=y_max_len, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 )
```

#### 4.1.2 dataloader


```python

# æµ‹è¯•å®šä¹‰çš„æ•°æ®é›†
BATCH_SIZE=128
train_loader = paddle.io.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)
val_loader=paddle.io.DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)
test_loader=paddle.io.DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)
example_x,xl, example_y ,yl = next(iter(val_loader))
print(example_x.shape)
print(example_y.shape)
print(example_y[:,:1].shape)
print(len(train_loader))
```

    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 50
    æ ·æœ¬æ•°ç›®ï¼š 34964
    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 50
    æ ·æœ¬æ•°ç›®ï¼š 9993
    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 50
    æ ·æœ¬æ•°ç›®ï¼š 5039
    [128, 240]
    [128, 50]
    [128, 1]
    273


### 2æ¨¡å‹ç»„ç½‘


```python
# æ³¨æ„åŠ›æœºåˆ¶ä½¿ç”¨çš„æ˜¯Bahdanauæå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—æ–¹æ³•
class Encoder(paddle.nn.Layer):
    def __init__(self, vocab_size, embed_dim, hidn_size,rate=0.2):
        super(Encoder, self).__init__()
        self.embedder = paddle.nn.Embedding(vocab_size, embed_dim)
        self.gru = paddle.nn.GRU(input_size=embed_dim,hidden_size=hidden_size,dropout=rate)
    def forward(self, sequence):
        inputs = self.embedder(sequence)
        encoder_output, encoder_state = self.gru(inputs)   
        # encoder_output [128, 18, 256]  [batch_size, time_steps, hidden_size]
        # encoder_state [num_layer*drection,batch_size,hidden_size]  num_layer*drection=1*1=1
        return encoder_output, encoder_state
class BahdanauAttention(paddle.nn.Layer):
    def __init__(self, hidden_size):
      super(BahdanauAttention, self).__init__()
      self.W1 = paddle.nn.Linear(hidden_size,hidden_size)
      self.W2 = paddle.nn.Linear(hidden_size,hidden_size)
      self.V = paddle.nn.Linear(hidden_size,1)
    def forward(self, hidden , encoder_out):
      # hidden éšè—å±‚çš„å½¢çŠ¶ == ï¼ˆ1,æ‰¹å¤§å°ï¼Œéšè—å±‚å¤§å°ï¼‰
      hidden = paddle.transpose(hidden, perm=[1, 0, 2]) #[batch_size,1,hidden_size]
      # encoder_out [batch_size,seq_len, hidden_size]
      # åˆ†æ•°çš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œæœ€å¤§é•¿åº¦ï¼Œ1ï¼‰
      # æˆ‘ä»¬åœ¨æœ€åä¸€ä¸ªè½´ä¸Šå¾—åˆ° 1ï¼Œ å› ä¸ºæˆ‘ä»¬æŠŠåˆ†æ•°åº”ç”¨äº self.V
      # åœ¨åº”ç”¨ self.V ä¹‹å‰ï¼Œå¼ é‡çš„å½¢çŠ¶æ˜¯ï¼ˆæ‰¹å¤§å°ï¼Œæœ€å¤§é•¿åº¦ï¼Œå•ä½ï¼‰
      score = self.V(paddle.nn.functional.tanh(self.W1(encoder_out) + self.W2(hidden)))
      # æ³¨æ„åŠ›æƒé‡ ï¼ˆattention_weightsï¼‰ çš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œæœ€å¤§é•¿åº¦ï¼Œ1ï¼‰
      attention_weights = paddle.nn.functional.softmax(score, axis=1)
      # ä¸Šä¸‹æ–‡å‘é‡ ï¼ˆcontext_vectorï¼‰ æ±‚å’Œä¹‹åçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œéšè—å±‚å¤§å°ï¼‰
      context_vector = attention_weights * encoder_out
      context_vector = paddle.sum(context_vector, axis=1)
      return context_vector
class Decoder(paddle.nn.Layer):
    def __init__(self, vocab_size, embedding_dim, hidden_size,rate=0.2):
        super(Decoder, self).__init__()
        self.embedding = paddle.nn.Embedding(vocab_size, embedding_dim)
        self.gru = paddle.nn.GRU(input_size=embedding_dim+hidden_size,hidden_size=hidden_size,dropout=rate)                          
        self.fc = paddle.nn.Linear(hidden_size,vocab_size)
        # ç”¨äºæ³¨æ„åŠ›
        self.attention = BahdanauAttention(hidden_size)
    def forward(self, x, hidden, enc_output):
        # ç¼–ç å™¨è¾“å‡º ï¼ˆenc_outputï¼‰ çš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œæœ€å¤§é•¿åº¦ï¼Œéšè—å±‚å¤§å°ï¼‰
        context_vector= self.attention(hidden, enc_output)  #[batch_size,hideen_size]
        # x åœ¨é€šè¿‡åµŒå…¥å±‚åçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œ1ï¼ŒåµŒå…¥ç»´åº¦ï¼‰
        x = self.embedding(x)
        # x åœ¨æ‹¼æ¥ ï¼ˆconcatenationï¼‰ åçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œ1ï¼ŒåµŒå…¥ç»´åº¦ + éšè—å±‚å¤§å°ï¼‰
        x = paddle.concat([paddle.unsqueeze(context_vector, 1), x], axis=-1)
        # å°†åˆå¹¶åçš„å‘é‡ä¼ é€åˆ° GRU
        output, state = self.gru(x)
        # è¾“å‡ºçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å° * 1ï¼Œéšè—å±‚å¤§å°ï¼‰
        output = paddle.reshape(output, (-1, output.shape[2]))
        # è¾“å‡ºçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œvocabï¼‰
        x = self.fc(output)
        return x, state
```

### 3é…ç½®è¶…å‚æ•°ï¼Œè®­ç»ƒæ¨¡å‹



```python
import pickle

def save_obj(obj, name ):
    with open('/home/aistudio/work/obj-'+name+'.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name ):
    with open('/home/aistudio/work/obj-' + name + '.pkl', 'rb') as f:
        return pickle.load(f)
word2id_dict=load_obj("word2id_dict")
id2word_dict=load_obj("id2word_dict")
```


```python
import time
EPOCHS = 10

embedding_size=256 # 
hidden_size=256 # 

max_grad_norm=5.0
learning_rate=0.001

train_batch_num=len(train_loader)
val_batch_num=len(val_loader)

vocab_size=len(id2word_dict)
droprate=0.1
encoder=Encoder(vocab_size,embedding_size,hidden_size,droprate)
decoder=Decoder(vocab_size,embedding_size,hidden_size,droprate)

# ä¼˜åŒ–å™¨
clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=max_grad_norm)
optim = paddle.optimizer.Adam(parameters=encoder.parameters()+decoder.parameters(),grad_clip=clip)
# è‡ªå®šä¹‰losså‡½æ•°ï¼Œæ¶ˆé™¤paddingçš„0çš„å½±å“
def getloss(predict, label):
    cost = paddle.nn.functional.cross_entropy(predict,label,reduction='none')
    zeo=paddle.zeros(label.shape,label.dtype)
    mask=paddle.cast(paddle.logical_not(paddle.equal(label,zeo)),dtype=predict.dtype)
    cost *=  mask
    return paddle.mean(cost)
```


```python
# è®­ç»ƒ
def train_step(inp, targ):
    loss = 0
    enc_output, enc_hidden = encoder(inp)
    dec_hidden = enc_hidden
    dec_input = paddle.unsqueeze(paddle.to_tensor([word2id_dict.get('<start>')] * BATCH_SIZE), 1)
    # æ•™å¸ˆå¼ºåˆ¶ - å°†ç›®æ ‡è¯ä½œä¸ºä¸‹ä¸€ä¸ªè¾“å…¥
    for t in range(1, targ.shape[1]):
        # å°†ç¼–ç å™¨è¾“å‡º ï¼ˆenc_outputï¼‰ ä¼ é€è‡³è§£ç å™¨
        predictions, dec_hidden= decoder(dec_input, dec_hidden, enc_output)
        loss += getloss(predictions,targ[:, t])
        # ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶
        dec_input =paddle.unsqueeze(targ[:, t], 1)
    batch_loss = (loss / int(targ.shape[1]))
    batch_loss.backward()
    optim.step()
    optim.clear_grad()
    return batch_loss
# éªŒè¯
def val_step(inp, targ):
    loss = 0
    enc_output, enc_hidden = encoder(inp)
    dec_hidden = enc_hidden
    dec_input = paddle.unsqueeze(paddle.to_tensor([word2id_dict.get('<start>')] * BATCH_SIZE), 1)
    # æ•™å¸ˆå¼ºåˆ¶ - å°†ç›®æ ‡è¯ä½œä¸ºä¸‹ä¸€ä¸ªè¾“å…¥
    for t in range(1, targ.shape[1]):
        # å°†ç¼–ç å™¨è¾“å‡º ï¼ˆenc_outputï¼‰ ä¼ é€è‡³è§£ç å™¨
        predictions, dec_hidden= decoder(dec_input, dec_hidden, enc_output)
        loss += getloss(predictions,targ[:, t])
        # ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶
        dec_input =paddle.unsqueeze(targ[:, t], 1)
    batch_loss = (loss / int(targ.shape[1]))
    # ä¸‹é¢è¿™è¡Œä¸èƒ½æ³¨é‡Šæ‰ï¼Œå¦åˆ™GPUçš„æ˜¾å­˜ä¼šçˆ†æ‰ï¼Œæœ‰å¤§ä½¬çŸ¥é“ä¸ºä»€ä¹ˆå—ï¼Ÿ
    batch_loss.backward()
    optim.clear_grad()
    return batch_loss
```


```python
train_loss_list=[]
val_loss_list=[]
EPOCHS = 10

def train():
    pre_dev_loss=1000000
    for epoch in range(EPOCHS):
        start = time.time()

        train_total_loss = 0

        encoder.train()
        decoder.train()

        for (batch, (inp,_, targ,_)) in enumerate(train_loader):
            batch_loss = train_step(inp, targ)
            train_total_loss += batch_loss

            if batch % 100 == 0:
                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()[0]))
        train_loss_list.append(train_total_loss.numpy()[0]/ train_batch_num)                                        
        print('train Epoch {} avaLoss {:.4f}'.format(epoch + 1,train_total_loss.numpy()[0] / train_batch_num))

        encoder.eval()
        decoder.eval()
        val_total_loss=0
        
        for (batch, (inp,_, targ,_)) in enumerate(val_loader):
            #print(batch,inp.shape,targ.shape)
            batch_loss = val_step(inp, targ)
        
            val_total_loss += batch_loss

            if batch % 20 == 0:
                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()[0]))
        val_loss_now=val_total_loss.numpy()[0] / val_batch_num
        if val_loss_now<pre_dev_loss:
            pre_dev_loss=val_loss_now
            paddle.save(encoder.state_dict(), "work/output/encoder.pdparams")
            paddle.save(decoder.state_dict(), "work/output/decoder.pdparams")
            paddle.save(optim.state_dict(), "work/output/optim.pdopt")
            print("stored!")

        val_loss_list.append(val_loss_now)                                                   
        print('val Epoch {} avaLoss {:.4f}'.format(epoch + 1,val_loss_now))
                
        print('Time taken for 1 epoch {}h\n'.format((time.time() - start)/3600))
```


```python
train() #æ³¨æ„ã€‚è¿™é‡Œä¸Šä¼ æƒé‡æ¥ç€è®­ç»ƒ
```

## äº” æ¨¡å‹è¯„ä¼°
åŒ…å«åœ¨è®­ç»ƒä¸­


```python
print(val_loss_list)
```

## å…¶ä»–éƒ¨åˆ†

### aistudioé“¾æ¥ (https://aistudio.baidu.com/aistudio/projectdetail/3571825)

### githubé“¾æ¥ (https://github.com/yuanyihan/aipp)

### giteeé“¾æ¥ (https://gitee.com/yuanyihan/aipp)



# ä¸ªäººä½œä¸šé“¾æ¥

## aistudioé“¾æ¥ (https://aistudio.baidu.com/aistudio/projectdetail/3571825)

## githubé“¾æ¥ (https://github.com/yuanyihan/aipp)

## giteeé“¾æ¥ (https://gitee.com/yuanyihan/aipp)


----
# ä»¥ä¸‹æ˜¯å›¢é˜Ÿç®¡ç†
----

## å‚è€ƒ
DWæ‰“å¡è®¡åˆ’ï¼šçŸ³å¢¨æ–‡æ¡£ https://shimo.im/sheets/v8YwYwtPhwprRxvg/MODOC/
é£æ¡¨é“¾æ¥åœ°å€ï¼šhttps://aistudio.baidu.com/aistudio/education/group/info/25259
é£æ¡¨æ¡†æ¶æ–‡æ¡£ï¼šhttps://www.paddlepaddle.org.cn/documentation/docs/zh/guides/01_paddle2.0_introduction/index_cn.html

---
## 21å¤©Check-Listï¼ˆåŒ…å«åˆ†ç»„æ‰“å¡è®¡åˆ’ï¼šD:dwæ‰“å¡   P:paddlepaddleæˆªå›¾æ‰“å¡ï¼‰
|æˆªæ­¢æ—¶é—´|è®¡åˆ’|å°é»‘|å°è›®å¦–|æ¡|draw|å›¾çµçš„çŒ«|Leo(ç¿”å“¥)|Strong|ä¸€å¤©|å¤‡æ³¨
|:----:|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|021224|åŠ å…¥é˜Ÿä¼|:heart:|:heart:|:heart:|:heart:|:heart:|:heart:|:heart:|:heart:|-|
|021324|å¡«å†™é—®å·|:heart:|:heart:|:heart:|:heart:|:heart:|:heart:|:heart:|:heart:|-|
|021503|[Task01ï¼šå…ˆå¯¼è¯¾ï¼šä½ æƒ³è¢«AIæ›¿ä»£ï¼Œè¿˜æ˜¯æˆä¸ºAIçš„åˆ›é€ è€…ï¼Ÿ](https://aistudio.baidu.com/aistudio/education/lessonvideo/2213406/1)|DP|DP|DP|DP|DP|DP|DP|DP|-|
|021603|[Task02ï¼šå¤´è„‘é£æš´ï¼šè®©äººæ‹æ¡ˆå«ç»çš„åˆ›æ„éƒ½æ˜¯å¦‚ä½•è¯ç”Ÿçš„ï¼Ÿ](https://aistudio.baidu.com/aistudio/education/lessonvideo/2215873/1)|DP|DP|DP|DP|DP|DP|DP|DP|-|
|021803|$^*$Task03ï¼šæ•°æ®å‡†å¤‡ï¼ˆ2é€‰1ï¼‰ï¼š[å›¾åƒå¤„ç†](https://aistudio.baidu.com/aistudio/education/lessonvideo/2223278/1)ä¸[æ–‡æœ¬å¤„ç†](https://aistudio.baidu.com/aistudio/education/lessonvideo/2223298/1)|DP|DP|DP|DP|DP|DP|DP|DP|-|
|021903|ä½œä¸š1|P|P|P|P|P|P|P|P|-|
|022103|[Task04ï¼šç†è®ºåŸºç¡€ï¼šæ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œç®—æ³•çš„åŸºæœ¬åŸç†ï¼ˆ3å¤©ï¼‰](https://aistudio.baidu.com/aistudio/education/lessonvideo/2230559)|DP|DP|DP|DP|DP|DP|DP|DP|-|
|022103|ä½œä¸š2|P|P|P|P|P|P|P|P|[è®²è§£è§†é¢‘](https://aistudio.baidu.com/aistudio/education/lessonvideo/2235328)|
|022203|[Task05ï¼šæ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨é£æ¡¨å·¥å…·ç»„ä»¶ä½æˆæœ¬å®Œæˆæ¨¡å‹è®­ç»ƒï¼ˆ1å¤©ï¼‰](https://aistudio.baidu.com/aistudio/education/lessonvideo/2239149)|DP|DP|DP|DP|DP|DP|DP|DP|-|
|022503|[Task06ï¼šæ¨¡å‹ä¼˜åŒ–ï¼šä»ç»“æ„ã€æ€§èƒ½ã€è®­ç»ƒã€è®¾è®¡æ–¹é¢ä¼˜åŒ–æ¨¡å‹ï¼ˆ3å¤©ï¼‰](https://aistudio.baidu.com/aistudio/education/lessonvideo/2242005)|DP|DP|DP|DP|DP|DP|DP|DP|-|
|022803|$^{**}$Task07ï¼šæ¨ç†éƒ¨ç½²ï¼ˆ5é€‰1ï¼‰ï¼šæœåŠ¡å™¨ã€è¾¹ç¼˜ç«¯ã€ç§»åŠ¨ç«¯éƒ¨ç½²å…¨è§£ï¼ˆ3å¤©ï¼‰|DP|DP|DP|DP|DP|DP|DP|DP|-|
|022803|ä½œä¸š3|P|P|P|P|P|P|P|P|-|
|030703|Task08ï¼šå¦‚ä½•æ’°å†™ç²¾é€‰é¡¹ç›®ï¼ˆ7å¤©ï¼‰|DP|DP|DP|DP|DP|DP|DP|DP|-|
|030703|ä½œä¸š4|P|P|P|P|P|P|P|P|-|

å¤‡æ³¨ï¼š
- $^*$ åˆ†ä¸º**å›¾**å’Œ**æ–‡**ï¼Œåˆ†åˆ«ä»£è¡¨**å›¾åƒå¤„ç†**å’Œ**æ–‡æœ¬å¤„ç†**
- $^{**}$ äº”ä¸ªï¼Œæ¯”è¾ƒå¥½ç©çš„æ˜¯å®‰å“å’ŒæœåŠ¡ç«¯

---
## æ‰“å¡è¯´æ˜
é£æ¡¨çš„ç§¯åˆ†è¯„ä¼˜è§„åˆ™ï¼š
è‡ªæˆ‘ä»‹ç»ï¼š5åˆ†
è¯¾ç¨‹åˆ†ï¼š8*10åˆ†
ä½œä¸šåˆ†ï¼š4*20åˆ†
é¢å¤–åŠ åˆ†ï¼šå…¨å‹¤+10ã€ä¼˜ç§€é˜Ÿé•¿+2åˆ†ã€è‡ªæœ‰åˆ›æ„+10ã€æ¨¡å‹è¿›é˜¶+10ã€ç²¾é€‰é¡¹ç›®+10
ç»“ä¸šæ¡ä»¶ï¼šå®Œæˆå¤§ä½œä¸šï¼ˆç¬¬å››æ¬¡ä½œä¸šï¼‰çš„åŸºç¡€ä¸Šï¼Œåˆ†æ•°è¾¾åˆ°120åˆ†è§†ä¸ºç»“ä¸š
è¯„ä¼˜æ¡ä»¶ï¼šå®Œæˆå¤§ä½œä¸šï¼ˆç¬¬å››æ¬¡ä½œä¸šï¼‰çš„åŸºç¡€ä¸Šï¼Œåˆ†æ•°è¾¾åˆ°160åˆ†è§†ä¸ºä¼˜ç§€
1ã€è¯¾ç¨‹ç§¯åˆ†ï¼šè§‚çœ‹é£æ¡¨å¹³å°è§†é¢‘è¯¾ç¨‹ï¼Œå¹¶ä¸”åœ¨datawhaleå°ç¨‹åºæˆªæ­¢æ—¶é—´å‰å‚åŠ æœ¬ç¾¤å†…æ¥é¾™å‘é€æˆªå›¾å¯è·å¾—æœ¬ä»»åŠ¡è¯¾ç¨‹åˆ†10åˆ†ï¼›
2ã€å…¨å‹¤å¥–åŠ±ï¼š8èŠ‚è¯¾ï¼ˆæ•°æ®å¤„ç†2é€‰1ï¼Œæ¨ç†éƒ¨ç½²5é€‰1ï¼‰å…¨å‹¤çš„å­¦å‘˜å¯ä»¥é¢å¤–åŠ 10åˆ†ï¼›
3ã€ä½œä¸šç§¯åˆ†ï¼š4æ¬¡ä½œä¸šåœ¨è§„å®šçš„æˆªæ­¢æ—¶é—´å‰ï¼ˆä¸æ˜¯é£æ¡¨å¹³å°ä½œä¸šç•Œé¢çš„æˆªæ­¢æ—¶é—´ï¼‰æäº¤é£æ¡¨å¹³å°å¯è·å¾—æœ¬æ¬¡ä½œä¸šåˆ†20åˆ†ï¼ˆæäº¤æ—¶é—´ç”±é£æ¡¨å¹³å°è‡ªåŠ¨ç»Ÿè®¡ï¼Œå„ä½å°ä¼™ä¼´æœ€å¥½ç•™å‡ºæ—¶é—´è£•é‡ï¼Œé¿å…å„ç§æ„å¤–æƒ…å†µå¯¼è‡´æäº¤è¶…æ—¶ï¼‰ï¼Œä¸Šä¸€æ¬¡ä½œä¸šæœªåœ¨è§„å®šæ—¶é—´å®Œæˆä¸å½±å“ä¸‹ä¸€æ¬¡ä½œä¸šæäº¤ï¼›
4ã€ä¸€å®šè¦æ³¨æ„ï¼ä½œä¸šè§„å®šæˆªæ­¢æ—¶é—´ä¸æ˜¯ä½œä¸šç•Œé¢ä¸Šçš„æˆªæ­¢æ—¶é—´ï¼è§„å®šæˆªæ­¢æ—¶é—´å°†åœ¨æ­£å¼å­¦ä¹ å¼€å§‹åé€šè¿‡ç¾¤å…¬å‘Šæå‰çº¦3å¤©å…¬å¸ƒï¼›
5ã€æ¯ä½åŒå­¦çš„ç§¯åˆ†å°†ç”±é¢†èˆªå‘˜å®šæœŸç»Ÿè®¡å¹¶å…¬å¸ƒï¼Œæœ‰å¼‚è®®è€…å¯åŠæ—¶ç¾¤å†…@é¢†èˆªå‘˜ï¼›
6ã€å„ä½åŠæ—¶å¡«å†™é£æ¡¨æ³¨å†Œæ˜µç§°æ”¶é›†é—®å·https://www.wjx.top/vm/P0Qs0Cc.aspx å¦åˆ™æ— æ³•ç»Ÿè®¡ä½œä¸šå®Œæˆæƒ…å†µï¼Œä¸èƒ½å‚ä¸é£æ¡¨ç§¯åˆ†è¯„å®šè¯„ä¼˜ï¼›
7ã€å¦‚æœä¹‹å‰å·²ç»åŠ å…¥é£æ¡¨å®˜æ–¹ç»„é˜Ÿç¾¤ï¼Œä¸é‡å¤å‚ä¸é£æ¡¨ç§¯åˆ†è¯„å®šè¯„ä¼˜ã€‚


---
è¿™æ¬¡å› ä¸ºéœ€è¦åœ¨é£æ¡¨å¹³å°æäº¤ä½œä¸šï¼Œæ‰€ä»¥ä¸å†è¦æ±‚å¤§å®¶åœ¨å°ç¨‹åºä¸­æäº¤ç¬”è®°é“¾æ¥~å„ä½å°ä¼™ä¼´æŒ‰ç…§æ—¶é—´èŠ‚ç‚¹æ‰“å¡å³å¯~
å†…å®¹è¦æ±‚ï¼š50å­—ä»¥ä¸Šè¯¾ç¨‹ç¬”è®°æˆ–å¿ƒå¾—ä½“ä¼šæ–‡å­—ç‰ˆ
æ‰“å¡æ—¶é—´ï¼šå‚è§å°ç¨‹åºä¸­å„ä»»åŠ¡æˆªæ­¢æ—¶é—´
Tipsï¼šå®Œæˆä¸Šä¸€ä»»åŠ¡æ‰“å¡æ‰ä¼šå¼€å¯ä¸‹ä¸€ä»»åŠ¡å“¦~
æœ¬æ¬¡å› ä¸ºé£æ¡¨çš„å­¦ä¹ ä»»åŠ¡åŒæ—¶è¿›è¡Œï¼Œdatawhaleå°ç¨‹åºæœªæ‰“å¡è€…ä¸ä¼šè¢«æŠ±å‡ºç¾¤ï¼Œä½†åç»­ä»»åŠ¡æ— æ³•ç»§ç»­æ‰“å¡ï¼Œå¤±å»é€€è¿˜ç›‘ç£é‡‘åŠè·å¾—ç»“è¥è¯ä¹¦èµ„æ ¼~
dwä¼˜ç§€å­¦ä¹ è€…è¯„é€‰ä»¥ç¾¤å†…å­¦ä¹ è®¨è®ºæƒ…å†µåŠé£æ¡¨ä½œä¸šå®Œæˆæƒ…å†µä¸ºä¾æ®ï¼Œä¼˜ç§€é˜Ÿé•¿è¯„é€‰å¢åŠ å°é˜Ÿç•™å­˜ç‡ã€å°é˜Ÿç»„ç»‡ç®¡ç†ã€ä¼˜ç§€ä½œä¸šæ¨èä¸‰é¡¹ã€‚

dwä¸è¦æ±‚æä¾›ç¬”è®°é“¾æ¥ã€‚ã€‚ã€‚ï¼ˆäº¤äº†ä¹Ÿä¸è¯„ï¼Œåªè¯„é£æ¡¨çš„4æ¬¡ä½œä¸šï¼‰
dwå’Œé£æ¡¨æ˜¯ä¸¤å¥—ä¸åŒçš„è¯„ä»·ç³»ç»Ÿï¼Œå„è‡ªå‘ç»“è¥å’Œä¼˜ç§€è¯ä¹¦~
