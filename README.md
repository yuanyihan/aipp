```python
# æŸ¥çœ‹å½“å‰æŒ‚è½½çš„æ•°æ®é›†ç›®å½•, è¯¥ç›®å½•ä¸‹çš„å˜æ›´é‡å¯ç¯å¢ƒåä¼šè‡ªåŠ¨è¿˜åŸ
# View dataset directory. 
# This directory will be recovered automatically after resetting environment. 
!ls /home/aistudio/data
```

    data128633



```python

```

# ä½œä¸šè¯„åˆ†è¯´æ˜
- [x] 1.æ ¼å¼è§„èŒƒï¼ˆæœ‰è‡³å°‘3ä¸ªå°æ ‡é¢˜ï¼Œå†…å®¹å®Œæ•´ï¼‰ï¼Œä¸€ä¸ªå°æ ‡é¢˜5åˆ†ï¼Œæœ€é«˜20åˆ†
- [x] 2.å›¾æ–‡å¹¶èŒ‚ï¼Œä¸€å¼ å›¾5åˆ†ï¼Œæœ€é«˜20åˆ†
- [x] 3.æœ‰å¯è¿è¡Œçš„ä»£ç ï¼Œä¸”ä»£ç å†…æœ‰è¯¦ç»†æ³¨é‡Šï¼Œ20åˆ†
- [x] 4.ä»£ç å¼€æºåˆ°githubï¼Œ15åˆ†
- [x] 5.ä»£ç åŒæ­¥åˆ°giteeï¼Œ5åˆ†

# ä½œä¸šå†…å®¹

## ä¸€ã€é¡¹ç›®èƒŒæ™¯ä»‹ç»

è¯¥éƒ¨åˆ†ä¸»è¦å‘å¤§å®¶ä»‹ç»ä½ çš„é¡¹ç›®ç›®å‰ç¤¾ä¼šç ”ç©¶æƒ…å†µï¼Œç ”ç©¶çƒ­åº¦ï¼Œæˆ–è€…ç”¨ç®€çŸ­ç²¾ç‚¼çš„è¯­è¨€è®©å¤§å®¶ç†è§£ä¸ºä»€ä¹ˆè¦åšå‡ºè¿™ä¸ªé¡¹ç›®
### 1.é€‰æ‹©[æ–‡æ¡ˆè¥é”€æ•°æ®é›†](https://aistudio.baidu.com/aistudio/datasetdetail/128633)æ•°æ®é›†
è‡ªå·±çš„æ•°æ®é›† æ•°æ®æ¥æºæ˜¯æ–‡æ¡ˆè¥é”€æ•°æ®é›†ï¼ˆ5ä¸‡ç»„æ•°æ®ï¼‰
### 2.é¡¹ç›®èƒŒæ™¯/æ„ä¹‰é‡å¤§
å¯¹äºç¤¾åª’è¥é”€ç³»ç»Ÿæ¥è¯´ï¼Œæ–‡æ¡ˆçš„ç¼–å†™æ˜¯è´¹äººè´¹äº‹çš„ï¼Œè€Œç°åœ¨NLPçš„ç”Ÿæˆå¾ˆå¼ºå¤§ï¼Œæ‰€ä»¥é‡‡ç”¨ç”Ÿæˆæ¨¡å‹å‡è½»äººå·¥çš„å·¥ä½œï¼ŒèŠ‚çœäººåŠ›æˆæœ¬ã€‚


## äºŒã€æ•°æ®ä»‹ç»
æ•°æ®æ¥æºæ˜¯æ–‡æ¡ˆè¥é”€æ•°æ®é›†ï¼ˆ5ä¸‡ç»„æ•°æ®ï¼‰ã€‚
### 1.æ€»è§ˆï¼š
æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
![å›¾ç‰‡.png](dataset.png)

### 2.æŸ¥çœ‹å’Œé¢„å¤„ç†

#### 2.2.1 linuxæŸ¥çœ‹æ•°æ®


```python
# æŸ¥çœ‹æ•°æ®
!ls /home/aistudio/data/data128633/
!ls /home/aistudio/work
!ls /home/aistudio/data/data128633/
!tree  /home/aistudio/data/data128633/
!head -21  /home/aistudio/data/data128633/æœé¥°æ•°æ®.json
```

    æœé¥°_50k.json  æœé¥°æ•°æ®.json
    æœé¥°_50k.json  æœé¥°æ•°æ®.json
    /home/aistudio/data/data128633/
    â”œâ”€â”€ æœé¥°_50k.json
    â””â”€â”€ æœé¥°æ•°æ®.json
    
    0 directories, 2 files
    {
        "1": {
            "title": "å·´æ‹‰å·´ æ‹‰ æ——ä¸‹ æ¢¦ å¤šå¤š ç«¥è£… ç”·ç«¥ æ¯›è¡« å†¬å­£ ä¸­å¤§ç«¥ æ¯›è¡« é»‘è‰²",
            "kb": {
                "é€‚ç”¨å­£èŠ‚": "å†¬å­£",
                "åšåº¦": "é€‚ä¸­",
                "é¢†å‹": "é«˜é¢†",
                "é€‚ç”¨å¹´é¾„": "9-12å²",
                "æè´¨æˆåˆ†": "é”¦çº¶",
                "å›¾æ¡ˆ": "å…¶å®ƒ",
                "ä¸Šå¸‚æ—¶é—´": "2018å†¬å­£",
                "é¢æ–™": "å…¶å®ƒ",
                "é£æ ¼": "ä¼‘é—²é£",
                "è¡£é—¨è¥Ÿ": "å¥—å¤´",
                "é€‚ç”¨æ€§åˆ«": "ç”·",
                "å®‰å…¨ç­‰çº§": "Bç±»",
                "æ¯›çº¿ç²—ç»†": "æ™®é€šæ¯›çº¿"
            },
            "ocr": "ä¸­å›½è“ï¼Œæ·±åœŸé»„ï¼Œå¥åº·å®‰å…¨ï¼ŒAé—¨è¥Ÿï¼Œé»‘è‰²ï¼Œè¡£è¢–ï¼Œé¢æ–™å±•ï¼Œäº§å“ä¿¡æ¯ï¼Œé¢†å£ï¼Œå¯æ°´æ´—ï¼Œç»†èŠ‚å±•ç¤ºï¼Œä¸å®œæš´æ™’ï¼Œä¸å¯æ¼‚ç™½ï¼ŒçŸ­æ‹‰é“¾è®¾è®¡ï¼Œç®€æ´å®ç”¨ï¼ŒåŠç‰Œä»·:239.00ï¼Œé€‚åˆå­£èŠ‚:ç§‹å†¬å­£ï¼ŒåŠå¼€é¢†è®¾è®¡ï¼Œèˆ’é€‚äº²è‚¤ï¼Œ]é¢æ–™æ„æˆï¼Œç”°å±æ€§è¯´æ˜ï¼Œä¸å¯æºè‡ªï¼Œåˆæ ¼è¯ï¼Œä¸å¯å¹²æµ",
            "reference": "ä¸‰åˆä¸€æ··çººçº±çº¿åˆ¶æˆï¼ŒæŸ”è½¯äº²è‚¤ï¼Œè´´èº«ç©¿ä¹Ÿæ²¡æœ‰æ‰æ„Ÿã€‚åŠå¼€é¢†çš„ç«‹é¢†è®¾è®¡ï¼Œåœ¨è¾ƒå‡‰çš„å¤©æ°”ï¼Œä¿æŠ¤è„–é¢ˆï¼Œç©¿è„±ä¹Ÿæ›´ä¸ºæ–¹ä¾¿ã€‚ä¾§è¢–çš„æ‹¼æ¥æ’è‰²è®¾è®¡ï¼Œå‡¸ç°ä¸ªæ€§ï¼Œå®å®ç©¿ä¸Šæ›´å¸…æ°”ã€‚"
        },


#### 2.2.2 æ•°æ®é¢„å¤„ç†å’Œåˆ†å‰²æ•°æ®é›†


```python
# åœ¨å¯è§†åŒ–ä¹‹å‰ï¼Œé¦–å…ˆå…ˆæŠŠæ•°æ®é¢„å¤„ç†ä¸€ä¸‹ï¼Œå¹¶åˆ†å‡ºæ•°æ®é›†
import json
import jieba
samples = set()
# Read json file.
json_path = '/home/aistudio/data/data128633/æœé¥°_50k.json'
with open(json_path, 'r', encoding='utf8') as file:
    jsf = json.load(file)
for jsobj in jsf.values():
    title = jsobj['title'] + ' '  # Get title.
    kb = dict(jsobj['kb']).items()  # Get attributes.
    kb_merged = ''
    for key, val in kb:
        kb_merged += key+' '+val+' '  # Merge attributes.
    ocr = ' '.join(list(jieba.cut(jsobj['ocr'])))  # Get OCR text.
    texts = []
    texts.append(title + ocr + kb_merged)  # Merge them.
    reference = ' '.join(list(jieba.cut(jsobj['reference'])))
    for text in texts:
        text=text.replace('\t',' ').replace('ã€','').replace('â€','').replace('(','').replace(')','').replace(';',' ').replace('â€œ',' ').replace('Â°',' ').replace('ï¼Œ','').replace('ã€‚\n',' ').replace(':',' ').replace('ã€‚',' ').replace('ï¼š','').replace('/',' ').replace('  ',' ').replace('  ',' ') 
        reference=reference.replace('\t',' ').replace('ã€','').replace('â€','').replace('(','').replace(')','').replace(';',' ').replace('â€œ',' ').replace('Â°',' ').replace('ï¼Œ','').replace('ã€‚\n',' ').replace(':',' ').replace('ã€‚',' ').replace('ï¼š','').replace('/',' ').replace('  ',' ').replace('  ',' ') 
        sample = text+'\t'+reference  # Seperate source and reference.
        samples.add(sample)
print(len(samples))
with open('/home/aistudio/data/data128633/æœé¥°_50k.txt', 'w', encoding='utf8') as file:
    for line in samples:
        file.write(line)
        file.write('\n')
import random
with open('/home/aistudio/work/train_50k.txt', 'w', encoding='utf8') as trainf, open('/home/aistudio/work/dev_50k.txt', 'w', encoding='utf8') as devf, open('/home/aistudio/work/test_50k.txt', 'w', encoding='utf8') as testf:
    for line in samples:
        tmp=random.random()
        if tmp<0.7:
            trainf.write(line)
            trainf.write('\n')
        elif  tmp<0.9:
            devf.write(line)
            devf.write('\n')
        else:
            testf.write(line)
            testf.write('\n')
```

    Building prefix dict from the default dictionary ...
    Dumping model to file cache /tmp/jieba.cache
    Loading model cost 0.864 seconds.
    Prefix dict has been built successfully.


    49996


#### 2.2.3 è¯äº‘å±•ç¤º


```python
!pip install wordcloud
# å®‰è£…è¯äº‘ï¼Œå¹¶æ‰“å°æ•°æ®ï¼ŒæŸ¥çœ‹ä¸€ä¸‹åˆ†å¸ƒ
```

    Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
    Collecting wordcloud
      Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1b/06/0516bdba2ebdc0d5bd476aa66f94666dd0ad6b9abda723fdf28e451db919/wordcloud-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (366 kB)
         |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366 kB 7.9 MB/s            
    [?25hRequirement already satisfied: pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from wordcloud) (8.2.0)
    Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from wordcloud) (2.2.3)
    Requirement already satisfied: numpy>=1.6.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from wordcloud) (1.19.5)
    Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (3.0.7)
    Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.2)
    Requirement already satisfied: six>=1.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.16.0)
    Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)
    Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (2019.3)
    Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.1.0)
    Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (56.2.0)
    Installing collected packages: wordcloud
    Successfully installed wordcloud-1.8.1
    [33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.
    You should consider upgrading via the '/opt/conda/envs/python35-paddle120-env/bin/python -m pip install --upgrade pip' command.[0m



```python
# jiebaè¿›è¡Œæ–‡æœ¬æ•°æ®çš„åˆ†è¯ #è¿™ä¸€æ­¥ä¸éœ€è¦åšï¼Œå› ä¸ºä¸Šé¢æ•°æ®å¤„ç†ä¸­å·²ç»æœ‰äº†
import jieba
from jieba import analyse
# è¯äº‘å¯è§†åŒ–
from wordcloud import WordCloud
import matplotlib.pyplot as plt  
with open('/home/aistudio/data/data128633/æœé¥°_50k.txt', mode='r', encoding='utf-8') as f:
    text = f.readlines()
def tongji(lines,index):    
    print(f'========ç¬¬{index}æ¡æ•°æ®=====')
    # print('/ '.join(lines[index].split(' ')))
    extract_tags = analyse.extract_tags(lines[index], withWeight=True)
    for i, j in extract_tags:
        pass #print(i, j)
    result = {}
    for word in extract_tags:
        result[word[0]] = word[1]
    wordcloud = WordCloud(
        background_color="white",
        max_font_size=50,
        font_path='/home/aistudio/work/simkai.ttf')
        #
    wordcloud.generate_from_frequencies(result)

    plt.figure()
    plt.axis('off')
    plt.imshow(wordcloud)
    plt.show()

# !pip install wordcloud
```


```python
tongji(text,1)
tongji(text,2)
tongji(text,3)
tongji(text,4)
tongji(text,5)
```

    ========ç¬¬1æ¡æ•°æ®=====



    
![png](output_13_1.png)
    


    ========ç¬¬2æ¡æ•°æ®=====



    
![png](output_13_3.png)
    


    ========ç¬¬3æ¡æ•°æ®=====



    
![png](output_13_5.png)
    


    ========ç¬¬4æ¡æ•°æ®=====



    
![png](output_13_7.png)
    


    ========ç¬¬5æ¡æ•°æ®=====



    
![png](output_13_9.png)
    


#### 2.2.4 ä¸ºæ¨¡å‹æ„å»ºè¯å…¸ç­‰ç»Ÿè®¡æ•°æ®


```python
#è¿™è¾¹å¤šç»Ÿè®¡ä¸€äº›æ•°æ®
# 50kçš„è¯æ±‡çš„dic
from collections import Counter
word_c={}
lenx=[]
leny=[]
word2count = Counter()
# with open('/home/aistudio/data/data128633/samæœé¥°æ•°æ®.txt', mode='r', encoding='utf-8') as f:
with open('/home/aistudio/data/data128633/æœé¥°_50k.txt', mode='r', encoding='utf-8') as f:
    texts = f.readlines()
    for line in texts:
        # print(line)
        line=line
        ss=line.split('\t')
        if len(ss)!=2:continue
        xlst,ylst=ss[0].strip().split(" ") ,ss[1].strip().split(" ")
        # print(xlst,ylst)
        lenx.append(len(xlst))
        leny.append(len(ylst))
        for i in xlst+ylst:
            if i in word_c.keys():
                word_c[i]=word_c[i]+1
            else:
                word_c[i]=1
print(max(lenx),max(leny))
print(len(word_c))
# å°†è¿™ä¸ªè¯å…¸ä¸­çš„è¯ï¼ŒæŒ‰ç…§å‡ºç°æ¬¡æ•°æ’åºï¼Œå‡ºç°æ¬¡æ•°è¶Šé«˜ï¼Œæ’åºè¶Šé å‰
# ä¸€èˆ¬æ¥è¯´ï¼Œå‡ºç°é¢‘ç‡é«˜çš„é«˜é¢‘è¯å¾€å¾€æ˜¯ï¼šIï¼Œtheï¼Œyouè¿™ç§ä»£è¯ï¼Œè€Œå‡ºç°é¢‘ç‡ä½çš„è¯ï¼Œå¾€å¾€æ˜¯ä¸€äº›åè¯ï¼Œå¦‚ï¼šnlp
word_freq_dict = sorted(word_c.items(), key = lambda x:x[1], reverse = True)
# æ„é€ 3ä¸ªä¸åŒçš„è¯å…¸ï¼Œåˆ†åˆ«å­˜å‚¨ï¼Œ
# æ¯ä¸ªè¯åˆ°idçš„æ˜ å°„å…³ç³»ï¼šword2id_dict
# æ¯ä¸ªidåˆ°è¯çš„æ˜ å°„å…³ç³»ï¼šid2word_dict
#word2id_dict.get('<start>')
word2id_dict = {'<pad>':0,'<unk>':1,'<start>':2}
id2word_dict = {0:'<pad>',1:'<unk>',2:'<start>'}
# æŒ‰ç…§é¢‘ç‡ï¼Œä»é«˜åˆ°ä½ï¼Œå¼€å§‹éå†æ¯ä¸ªå•è¯ï¼Œå¹¶ä¸ºè¿™ä¸ªå•è¯æ„é€ ä¸€ä¸ªç‹¬ä¸€æ— äºŒçš„id
frequency=4 #å…±è®¡95755ä¸ªè¯ï¼Œè¶…è¿‡4ä¸ªé¢‘ç‡çš„è¯æ˜¯31609
for word, freq in word_freq_dict:
    if freq>frequency:
        curr_id = len(word2id_dict)
        word2id_dict[word] = curr_id
        id2word_dict[curr_id] = word
    else:
        word2id_dict[word]=1
print("è¶…è¿‡4ä¸ªé¢‘ç‡çš„è¯:",len(id2word_dict)-3)
print("å…±è®¡:",len(word2id_dict)-3)
print("å‰5ä¸ªé«˜é¢‘è¯ï¼š",word_freq_dict[:5])
# baocun
import pickle
def save_obj(obj, name ):
    with open('/home/aistudio/work/obj-'+name+'.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name ):
    with open('/home/aistudio/work/obj-' + name + '.pkl', 'rb') as f:
        return pickle.load(f)
save_obj(word_freq_dict,"word_freq_dict")
save_obj(word2id_dict,"word2id_dict")
save_obj(id2word_dict,"id2word_dict")
print(load_obj("word_freq_dict")[:6])
```

    276 56
    95755
    è¶…è¿‡4ä¸ªé¢‘ç‡çš„è¯: 31609
    å…±è®¡: 95755
    å‰5ä¸ªé«˜é¢‘è¯ï¼š [('çš„', 324132), ('è®¾è®¡', 111486), ('é¢æ–™', 107355), ('èˆ’é€‚', 86106), ('å±•ç¤º', 59808)]
    [('çš„', 324132), ('è®¾è®¡', 111486), ('é¢æ–™', 107355), ('èˆ’é€‚', 86106), ('å±•ç¤º', 59808), ('æŒ‡æ•°', 55554)]


### 3.æ•°æ®é›†ç±»çš„å®šä¹‰ï¼ˆç»§æ‰¿paddle.io.Datasetçš„ç±»ï¼‰



```python
import paddle
print(paddle.vision)
```

    <module 'paddle.vision' from '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/vision/__init__.py'>



```python
import sys 
sys.path.append('/home/aistudio/external-libraries')
import numpy as np
import pickle
import paddle
import paddle.vision.transforms as T
print("---")
class MyYingxiaoDataset(paddle.io.Dataset):
    def __init__(self,
                 file_path='/home/aistudio/data/data128633/æœé¥°æ•°æ®_50k.txt', #æŠ½æ ·
                 x_max_len=240, # xæœ€å¤§é•¿åº¦
                 y_max_len=50, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 ):
        super(MyYingxiaoDataset, self).__init__()
        self.x =[]  # xæ•°æ®
        self.y=[] #yæ•°æ®
        self.xl =[]  # xæœ‰æ•ˆé•¿åº¦æ•°æ®
        self.yl= [] #yæœ‰æ•ˆé•¿åº¦æ•°æ®
        word2id_dict={}
        print("é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š",x_max_len,y_max_len)
        with open(word2id_dict_pkl_path, 'rb') as f:
            word2id_dict= pickle.load(f)
        with open(file_path, mode='r', encoding='utf-8') as f:
        # with open('/home/aistudio/data/data128633/æœé¥°_50k.txt', mode='r', encoding='utf-8') as f:
            texts = f.readlines()
            for line in texts:
                # print(line)
                line=line
                ss=line.split('\t')
                if len(ss)!=2:continue
                xlst,ylst=ss[0].strip().split(" ") ,ss[1].strip().split(" ")
                xid=[ word2id_dict.get(v, 0)  for v in xlst]+[0]*x_max_len
                yid=[ word2id_dict.get(v, 0)  for v in ylst]+[0]*y_max_len
                self.x.append(xid[:x_max_len])
                self.y.append(yid[:y_max_len])
                self.xl.append(min(x_max_len,len(xlst)))
                self.yl.append(min(y_max_len,len(ylst)))
        self.lenall=len(self.xl)
        print('æ ·æœ¬æ•°ç›®ï¼š',self.lenall)

    def __getitem__(self, index):
        return paddle.to_tensor(self.x[index]),paddle.to_tensor(self.xl[index]),paddle.to_tensor(self.y[index]),paddle.to_tensor(self.yl[index])

    def __len__(self):
        return self.lenall
```

    ---



```python
print(paddle.vision)
file_path='/home/aistudio/work/dev_50k.txt'
x_max_len=240 # xæœ€å¤§é•¿åº¦
y_max_len=30 # yæœ€å¤§é•¿åº¦
train_dataset = MyYingxiaoDataset( file_path=file_path,
                 x_max_len=x_max_len, # xæœ€å¤§é•¿åº¦
                 y_max_len=y_max_len, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 )

index=9
with open(file_path, mode='r', encoding='utf-8') as f:
    texts = f.readlines()[index]
xs,ys=texts.split('\t')
print("é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š",x_max_len,y_max_len)
print("åŸå§‹æŸæ¡æ•°æ®çš„XYé•¿åº¦ï¼š",len(xs.strip().split(" ")),len(ys.strip().split(" ")))
x,xl,y,yl = train_dataset[index]
print("DatasetæŸæ¡æ•°æ®çš„XYé•¿åº¦ï¼š",xl,yl)
print("DatasetæŸæ¡å¡«å……æˆ–æˆªæ–­åæ•°æ®çš„XYé•¿åº¦ï¼š",x.shape,y.shape)
print(len(train_dataset))
print(x,xl,y,yl)
for x,xl,y,yl in train_dataset:
    print(x.shape,xl.shape,y.shape,yl.shape)
    break
```

    <module 'paddle.vision' from '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/vision/__init__.py'>
    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 30
    æ ·æœ¬æ•°ç›®ï¼š 9993
    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 30
    åŸå§‹æŸæ¡æ•°æ®çš„XYé•¿åº¦ï¼š 211 29
    DatasetæŸæ¡æ•°æ®çš„XYé•¿åº¦ï¼š Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
           [211]) Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
           [29])
    DatasetæŸæ¡å¡«å……æˆ–æˆªæ–­åæ•°æ®çš„XYé•¿åº¦ï¼š [240] [30]
    9993
    Tensor(shape=[240], dtype=int64, place=CPUPlace, stop_gradient=True,
           [3263, 755 , 65  , 1446, 308 , 362 , 56  , 371 , 24  , 1380, 659 , 2935,
            35  , 3263, 113 , 2223, 127 , 906 , 829 , 1931, 5211, 869 , 2577, 2101,
            13  , 7   , 25  , 374 , 25  , 7   , 351 , 2184, 906 , 829 , 251 , 26  ,
            897 , 709 , 2977, 1225, 2730, 3225, 386 , 327 , 43  , 613 , 1877, 734 ,
            740 , 144 , 659 , 859 , 177 , 96  , 1908, 38  , 19  , 276 , 280 , 10  ,
            122 , 906 , 829 , 47  , 4   , 357 , 3   , 145 , 55  , 336 , 507 , 3   ,
            56  , 561 , 495 , 351 , 210 , 225 , 829 , 1312, 1181, 64  , 29  , 1931,
            659 , 423 , 2545, 46  , 76  , 1027, 717 , 829 , 155 , 166 , 11  , 24  ,
            3   , 113 , 280 , 96  , 109 , 285 , 1225, 6084, 2730, 3225, 5532, 27  ,
            4054, 1089, 7120, 1457, 131 , 48  , 1793, 829 , 613 , 2492, 229 , 12758,
            2003, 911 , 15  , 452 , 18117, 10746, 4   , 6819, 87  , 538 , 567 , 8504,
            210 , 225 , 1312, 10747, 14  , 63  , 27  , 332 , 633 , 56  , 755 , 2998,
            11104, 900 , 1145, 6001, 1724, 8138, 611 , 3   , 6214, 38  , 20638, 3   ,
            9739, 8822, 1297, 18854, 4   , 1454, 33  , 1   , 35  , 1312, 277 , 829 ,
            277 , 3818, 11104, 12051, 917 , 14575, 1285, 1348, 18118, 906 , 43  , 351 ,
            2231, 13558, 475 , 33  , 2088, 10145, 3   , 28634, 369 , 14229, 41  , 1   ,
            3423, 148 , 4769, 2012, 3197, 27  , 3263, 2742, 2250, 2170, 3887, 21  ,
            490 , 44  , 160 , 2284, 1   , 2261, 15349, 0   , 0   , 0   , 0   , 0   ,
            0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   ,
            0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   , 0   ]) Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
           [211]) Tensor(shape=[30], dtype=int64, place=CPUPlace, stop_gradient=True,
           [29 , 11 , 127, 906, 829, 4  , 650, 3968, 346, 131, 7121, 606, 709, 3  ,
            3115, 155, 371, 713, 492, 1446, 308, 3  , 63 , 1227, 112, 4167, 538, 159,
            6  , 0  ]) Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
           [29])
    [240] [1] [30] [1]


## å›› æ¨¡å‹è®­ç»ƒ
### 1.æ•°æ®å¤„ç†

#### 4.1.1 æ„å»ºdataset


```python
# é¦–å…ˆï¼Œæ•°æ®å¤„ç†ä¸€ä¸‹
x_max_len=240 # xæœ€å¤§é•¿åº¦
y_max_len=50 # yæœ€å¤§é•¿åº¦
train_dataset = MyYingxiaoDataset( file_path='/home/aistudio/work/train_50k.txt',
                 x_max_len=x_max_len, # xæœ€å¤§é•¿åº¦
                 y_max_len=y_max_len, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 )
val_dataset = MyYingxiaoDataset( file_path='/home/aistudio/work/dev_50k.txt',
                 x_max_len=x_max_len, # xæœ€å¤§é•¿åº¦
                 y_max_len=y_max_len, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 )
test_dataset = MyYingxiaoDataset( file_path='/home/aistudio/work/test_50k.txt',
                 x_max_len=x_max_len, # xæœ€å¤§é•¿åº¦
                 y_max_len=y_max_len, # yæœ€å¤§é•¿åº¦
                 word2id_dict_pkl_path='work/obj-word2id_dict.pkl' # pklçš„è·¯å¾„
                 )
```

    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 50
    æ ·æœ¬æ•°ç›®ï¼š 34964
    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 50
    æ ·æœ¬æ•°ç›®ï¼š 9993
    é¢„è®¾å¡«å……æˆ–æˆªæ–­çš„XYé•¿åº¦ï¼š 240 50
    æ ·æœ¬æ•°ç›®ï¼š 5039


#### 4.1.2 dataloader


```python

# æµ‹è¯•å®šä¹‰çš„æ•°æ®é›†
BATCH_SIZE=256
train_loader = paddle.io.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)
val_loader=paddle.io.DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)
test_loader=paddle.io.DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True,drop_last=True)
example_x,xl, example_y ,yl = next(iter(val_loader))
print(example_x.shape)
print(example_y.shape)
print(example_y[:,:1].shape)
train_batch_num=len(train_loader)
val_batch_num=len(val_loader)
test_batch_num=len(test_loader)
```

    [256, 240]
    [256, 50]
    [256, 1]


### 2æ¨¡å‹ç»„ç½‘


```python
# æ³¨æ„åŠ›æœºåˆ¶ä½¿ç”¨çš„æ˜¯Bahdanauæå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—æ–¹æ³•
class Encoder(paddle.nn.Layer):
    def __init__(self, vocab_size, embed_dim, hidn_size,rate=0.2):
        super(Encoder, self).__init__()
        self.embedder = paddle.nn.Embedding(vocab_size, embed_dim)
        self.gru = paddle.nn.GRU(input_size=embed_dim,hidden_size=hidden_size,dropout=rate)
    def forward(self, sequence):
        inputs = self.embedder(sequence)
        encoder_output, encoder_state = self.gru(inputs)   
        # encoder_output [128, 18, 256]  [batch_size, time_steps, hidden_size]
        # encoder_state [num_layer*drection,batch_size,hidden_size]  num_layer*drection=1*1=1
        return encoder_output, encoder_state
class BahdanauAttention(paddle.nn.Layer):
    def __init__(self, hidden_size):
      super(BahdanauAttention, self).__init__()
      self.W1 = paddle.nn.Linear(hidden_size,hidden_size)
      self.W2 = paddle.nn.Linear(hidden_size,hidden_size)
      self.V = paddle.nn.Linear(hidden_size,1)
    def forward(self, hidden , encoder_out):
      # hidden éšè—å±‚çš„å½¢çŠ¶ == ï¼ˆ1,æ‰¹å¤§å°ï¼Œéšè—å±‚å¤§å°ï¼‰
      hidden = paddle.transpose(hidden, perm=[1, 0, 2]) #[batch_size,1,hidden_size]
      # encoder_out [batch_size,seq_len, hidden_size]
      # åˆ†æ•°çš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œæœ€å¤§é•¿åº¦ï¼Œ1ï¼‰
      # æˆ‘ä»¬åœ¨æœ€åä¸€ä¸ªè½´ä¸Šå¾—åˆ° 1ï¼Œ å› ä¸ºæˆ‘ä»¬æŠŠåˆ†æ•°åº”ç”¨äº self.V
      # åœ¨åº”ç”¨ self.V ä¹‹å‰ï¼Œå¼ é‡çš„å½¢çŠ¶æ˜¯ï¼ˆæ‰¹å¤§å°ï¼Œæœ€å¤§é•¿åº¦ï¼Œå•ä½ï¼‰
      score = self.V(paddle.nn.functional.tanh(self.W1(encoder_out) + self.W2(hidden)))
      # æ³¨æ„åŠ›æƒé‡ ï¼ˆattention_weightsï¼‰ çš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œæœ€å¤§é•¿åº¦ï¼Œ1ï¼‰
      attention_weights = paddle.nn.functional.softmax(score, axis=1)
      # ä¸Šä¸‹æ–‡å‘é‡ ï¼ˆcontext_vectorï¼‰ æ±‚å’Œä¹‹åçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œéšè—å±‚å¤§å°ï¼‰
      context_vector = attention_weights * encoder_out
      context_vector = paddle.sum(context_vector, axis=1)
      return context_vector
class Decoder(paddle.nn.Layer):
    def __init__(self, vocab_size, embedding_dim, hidden_size,rate=0.2):
        super(Decoder, self).__init__()
        self.embedding = paddle.nn.Embedding(vocab_size, embedding_dim)
        self.gru = paddle.nn.GRU(input_size=embedding_dim+hidden_size,hidden_size=hidden_size,dropout=rate)                          
        self.fc = paddle.nn.Linear(hidden_size,vocab_size)
        # ç”¨äºæ³¨æ„åŠ›
        self.attention = BahdanauAttention(hidden_size)
    def forward(self, x, hidden, enc_output):
        # ç¼–ç å™¨è¾“å‡º ï¼ˆenc_outputï¼‰ çš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œæœ€å¤§é•¿åº¦ï¼Œéšè—å±‚å¤§å°ï¼‰
        context_vector= self.attention(hidden, enc_output)  #[batch_size,hideen_size]
        # x åœ¨é€šè¿‡åµŒå…¥å±‚åçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œ1ï¼ŒåµŒå…¥ç»´åº¦ï¼‰
        x = self.embedding(x)
        # x åœ¨æ‹¼æ¥ ï¼ˆconcatenationï¼‰ åçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œ1ï¼ŒåµŒå…¥ç»´åº¦ + éšè—å±‚å¤§å°ï¼‰
        x = paddle.concat([paddle.unsqueeze(context_vector, 1), x], axis=-1)
        # å°†åˆå¹¶åçš„å‘é‡ä¼ é€åˆ° GRU
        output, state = self.gru(x)
        # è¾“å‡ºçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å° * 1ï¼Œéšè—å±‚å¤§å°ï¼‰
        output = paddle.reshape(output, (-1, output.shape[2]))
        # è¾“å‡ºçš„å½¢çŠ¶ == ï¼ˆæ‰¹å¤§å°ï¼Œvocabï¼‰
        x = self.fc(output)
        return x, state
```

### 3é…ç½®è¶…å‚æ•°ï¼Œè®­ç»ƒæ¨¡å‹



```python
import pickle

def save_obj(obj, name ):
    with open('/home/aistudio/work/obj-'+name+'.pkl', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name ):
    with open('/home/aistudio/work/obj-' + name + '.pkl', 'rb') as f:
        return pickle.load(f)
word2id_dict=load_obj("word2id_dict")
id2word_dict=load_obj("id2word_dict")
```


```python
import time

embedding_size=256 # 
hidden_size=256 # 

max_grad_norm=5.0
learning_rate=0.001



vocab_size=len(id2word_dict)
droprate=0.2
encoder=Encoder(vocab_size,embedding_size,hidden_size,droprate)
decoder=Decoder(vocab_size,embedding_size,hidden_size,droprate)
# encoder.set_state_dict(paddle.load("work/output/encoder.pdparams"))
# decoder.set_state_dict(paddle.load("work/output/decoder.pdparams"))
# ä¼˜åŒ–å™¨
clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=max_grad_norm)
optim = paddle.optimizer.Adam(parameters=encoder.parameters()+decoder.parameters(),grad_clip=clip)

# optim.set_state_dict(paddle.load("work/output/optim.pdopt"))


# è‡ªå®šä¹‰losså‡½æ•°ï¼Œæ¶ˆé™¤paddingçš„0çš„å½±å“
def getloss(predict, label):
    cost = paddle.nn.functional.cross_entropy(predict,label,reduction='none')
    zeo=paddle.zeros(label.shape,label.dtype)
    mask=paddle.cast(paddle.logical_not(paddle.equal(label,zeo)),dtype=predict.dtype)
    cost *=  mask
    return paddle.mean(cost)
```


```python
# è®­ç»ƒ
def train_step(inp, targ):
    loss = 0
    enc_output, enc_hidden = encoder(inp)
    dec_hidden = enc_hidden
    dec_input = paddle.unsqueeze(paddle.to_tensor([word2id_dict.get('<start>')] * BATCH_SIZE), 1)
    # æ•™å¸ˆå¼ºåˆ¶ - å°†ç›®æ ‡è¯ä½œä¸ºä¸‹ä¸€ä¸ªè¾“å…¥
    for t in range(1, targ.shape[1]):
        # å°†ç¼–ç å™¨è¾“å‡º ï¼ˆenc_outputï¼‰ ä¼ é€è‡³è§£ç å™¨
        predictions, dec_hidden= decoder(dec_input, dec_hidden, enc_output)
        loss += getloss(predictions,targ[:, t])
        # ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶
        dec_input =paddle.unsqueeze(targ[:, t], 1)
    batch_loss = (loss / int(targ.shape[1]))
    batch_loss.backward()
    optim.step()
    optim.clear_grad()
    return batch_loss
# éªŒè¯
def val_step(inp, targ):
    loss = 0
    enc_output, enc_hidden = encoder(inp)
    dec_hidden = enc_hidden
    dec_input = paddle.unsqueeze(paddle.to_tensor([word2id_dict.get('<start>')] * BATCH_SIZE), 1)
    # æ•™å¸ˆå¼ºåˆ¶ - å°†ç›®æ ‡è¯ä½œä¸ºä¸‹ä¸€ä¸ªè¾“å…¥
    for t in range(1, targ.shape[1]):
        # å°†ç¼–ç å™¨è¾“å‡º ï¼ˆenc_outputï¼‰ ä¼ é€è‡³è§£ç å™¨
        predictions, dec_hidden= decoder(dec_input, dec_hidden, enc_output)
        loss += getloss(predictions,targ[:, t])
        # ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶
        dec_input =paddle.unsqueeze(targ[:, t], 1)
    batch_loss = (loss / int(targ.shape[1]))
    # ä¸‹é¢è¿™è¡Œä¸èƒ½æ³¨é‡Šæ‰ï¼Œå¦åˆ™GPUçš„æ˜¾å­˜ä¼šçˆ†æ‰ï¼Œæœ‰å¤§ä½¬çŸ¥é“ä¸ºä»€ä¹ˆå—ï¼Ÿ
    batch_loss.backward()
    optim.clear_grad()
    return batch_loss
```


```python
train_loss_list=[]
val_loss_list=[]
EPOCHS = 100

def train():
    early_stop_cnt=0
    pre_dev_loss=1000000
    for epoch in range(EPOCHS):
        start = time.time()

        train_total_loss = 0

        encoder.train()
        decoder.train()

        for (batch, (inp,_, targ,_)) in enumerate(train_loader):
            batch_loss = train_step(inp, targ)
            train_total_loss += batch_loss

            if batch % 100 == 0:
                print('train Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()[0]))
        train_loss_list.append(train_total_loss.numpy()[0]/ train_batch_num)                                        
        print('train Epoch {} avaLoss {:.4f}'.format(epoch + 1,train_total_loss.numpy()[0] / train_batch_num))

        encoder.eval()
        decoder.eval()
        val_total_loss=0
        
        for (batch, (inp,_, targ,_)) in enumerate(val_loader):
            #print(batch,inp.shape,targ.shape)
            batch_loss = val_step(inp, targ)
        
            val_total_loss += batch_loss

            if batch % 20 == 0:
                print('val Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch,batch_loss.numpy()[0]))
        val_loss_now=val_total_loss.numpy()[0] / val_batch_num
        if val_loss_now<=pre_dev_loss:
            pre_dev_loss=val_loss_now
            paddle.save(encoder.state_dict(), "work/output/encoder.pdparams")
            paddle.save(decoder.state_dict(), "work/output/decoder.pdparams")
            paddle.save(optim.state_dict(), "work/output/optim.pdopt")
            print("stored!")
            early_stop_cnt=0
        else:
            early_stop_cnt+=1
            

        val_loss_list.append(val_loss_now)                                                   
        print('val Epoch {} avaLoss {:.4f}'.format(epoch + 1,val_loss_now))
                
        print('Time taken for 1 epoch {}h\n'.format((time.time() - start)/3600))
        if early_stop_cnt>=5:
            print("5è½®å‚æ•°æ²¡æœ‰ä¼˜åŒ–ï¼Œæ—©åœ")
            return 0
```


```python
train() #æ³¨æ„ã€‚è¿™é‡Œä¸Šä¼ æƒé‡æ¥ç€è®­ç»ƒ
# 
```

    train Epoch 1 Batch 0 Loss 7.7245
    train Epoch 1 Batch 100 Loss 5.1874
    train Epoch 1 avaLoss 5.4012
    val Epoch 1 Batch 0 Loss 5.0995
    val Epoch 1 Batch 20 Loss 5.0819
    stored!
    val Epoch 1 avaLoss 5.1076
    Time taken for 1 epoch 0.07543666270044115h
    
    train Epoch 2 Batch 0 Loss 5.0824
    train Epoch 2 Batch 100 Loss 5.0699
    train Epoch 2 avaLoss 5.0481
    val Epoch 2 Batch 0 Loss 4.9052
    val Epoch 2 Batch 20 Loss 5.0982
    stored!
    val Epoch 2 avaLoss 4.9817
    Time taken for 1 epoch 0.07549830502933925h
    
    train Epoch 3 Batch 0 Loss 4.9577
    train Epoch 3 Batch 100 Loss 4.9061
    train Epoch 3 avaLoss 4.9141
    val Epoch 3 Batch 0 Loss 4.8261
    val Epoch 3 Batch 20 Loss 4.8940
    stored!
    val Epoch 3 avaLoss 4.8209
    Time taken for 1 epoch 0.07543086859914992h
    
    train Epoch 4 Batch 0 Loss 4.9052
    train Epoch 4 Batch 100 Loss 4.6428
    train Epoch 4 avaLoss 4.7499
    val Epoch 4 Batch 0 Loss 4.5386
    val Epoch 4 Batch 20 Loss 4.7438
    stored!
    val Epoch 4 avaLoss 4.6759
    Time taken for 1 epoch 0.07554376231299506h
    
    train Epoch 5 Batch 0 Loss 4.6608
    train Epoch 5 Batch 100 Loss 4.5538
    train Epoch 5 avaLoss 4.5867
    val Epoch 5 Batch 0 Loss 4.4663
    val Epoch 5 Batch 20 Loss 4.5226
    stored!
    val Epoch 5 avaLoss 4.5248
    Time taken for 1 epoch 0.07556546853648291h
    
    train Epoch 6 Batch 0 Loss 4.4988
    train Epoch 6 Batch 100 Loss 4.4255
    train Epoch 6 avaLoss 4.4350
    val Epoch 6 Batch 0 Loss 4.4450
    val Epoch 6 Batch 20 Loss 4.3508
    stored!
    val Epoch 6 avaLoss 4.3926
    Time taken for 1 epoch 0.07558300428920323h
    
    train Epoch 7 Batch 0 Loss 4.3056
    train Epoch 7 Batch 100 Loss 4.2970
    train Epoch 7 avaLoss 4.2959
    val Epoch 7 Batch 0 Loss 4.2327
    val Epoch 7 Batch 20 Loss 4.3385
    stored!
    val Epoch 7 avaLoss 4.2739
    Time taken for 1 epoch 0.07539850387308333h
    
    train Epoch 8 Batch 0 Loss 4.2248
    train Epoch 8 Batch 100 Loss 4.0756
    train Epoch 8 avaLoss 4.1726
    val Epoch 8 Batch 0 Loss 4.1304
    val Epoch 8 Batch 20 Loss 4.2228
    stored!
    val Epoch 8 avaLoss 4.1729
    Time taken for 1 epoch 0.07558588100804223h
    
    train Epoch 9 Batch 0 Loss 4.1295
    train Epoch 9 Batch 100 Loss 3.9992
    train Epoch 9 avaLoss 4.0651
    val Epoch 9 Batch 0 Loss 4.0970
    val Epoch 9 Batch 20 Loss 4.0490
    stored!
    val Epoch 9 avaLoss 4.0873
    Time taken for 1 epoch 0.07570075478818682h
    
    train Epoch 10 Batch 0 Loss 3.9145
    train Epoch 10 Batch 100 Loss 3.8788
    train Epoch 10 avaLoss 3.9678
    val Epoch 10 Batch 0 Loss 4.0887
    val Epoch 10 Batch 20 Loss 3.9508
    stored!
    val Epoch 10 avaLoss 4.0112
    Time taken for 1 epoch 0.07558037850591871h
    
    train Epoch 11 Batch 0 Loss 3.8690
    train Epoch 11 Batch 100 Loss 3.8369
    train Epoch 11 avaLoss 3.8779
    val Epoch 11 Batch 0 Loss 3.9301
    val Epoch 11 Batch 20 Loss 3.9345
    stored!
    val Epoch 11 avaLoss 3.9458
    Time taken for 1 epoch 0.07566915147834354h
    
    train Epoch 12 Batch 0 Loss 3.8249
    train Epoch 12 Batch 100 Loss 3.7435
    train Epoch 12 avaLoss 3.8000
    val Epoch 12 Batch 0 Loss 3.8343
    val Epoch 12 Batch 20 Loss 3.9292
    stored!
    val Epoch 12 avaLoss 3.8937
    Time taken for 1 epoch 0.07562061058150397h
    
    train Epoch 13 Batch 0 Loss 3.7352
    train Epoch 13 Batch 100 Loss 3.6690
    train Epoch 13 avaLoss 3.7299
    val Epoch 13 Batch 0 Loss 3.8415
    val Epoch 13 Batch 20 Loss 3.9443
    stored!
    val Epoch 13 avaLoss 3.8490
    Time taken for 1 epoch 0.07577338099479675h
    
    train Epoch 14 Batch 0 Loss 3.6082
    train Epoch 14 Batch 100 Loss 3.6322
    train Epoch 14 avaLoss 3.6680
    val Epoch 14 Batch 0 Loss 3.8937
    val Epoch 14 Batch 20 Loss 3.8442
    stored!
    val Epoch 14 avaLoss 3.8131
    Time taken for 1 epoch 0.07567441854212019h
    
    train Epoch 15 Batch 0 Loss 3.5542
    train Epoch 15 Batch 100 Loss 3.5464
    train Epoch 15 avaLoss 3.6114
    val Epoch 15 Batch 0 Loss 3.8138
    val Epoch 15 Batch 20 Loss 3.9335
    stored!
    val Epoch 15 avaLoss 3.7826
    Time taken for 1 epoch 0.07555800358454387h
    
    train Epoch 16 Batch 0 Loss 3.6613
    train Epoch 16 Batch 100 Loss 3.5187
    train Epoch 16 avaLoss 3.5607
    val Epoch 16 Batch 0 Loss 3.9109
    val Epoch 16 Batch 20 Loss 3.7931
    stored!
    val Epoch 16 avaLoss 3.7573
    Time taken for 1 epoch 0.0756335840622584h
    
    train Epoch 17 Batch 0 Loss 3.4601
    train Epoch 17 Batch 100 Loss 3.5430
    train Epoch 17 avaLoss 3.5131
    val Epoch 17 Batch 0 Loss 3.7189
    val Epoch 17 Batch 20 Loss 3.7970
    stored!
    val Epoch 17 avaLoss 3.7403
    Time taken for 1 epoch 0.07565581580003103h
    
    train Epoch 18 Batch 0 Loss 3.4518
    train Epoch 18 Batch 100 Loss 3.3952
    train Epoch 18 avaLoss 3.4688
    val Epoch 18 Batch 0 Loss 3.6759
    val Epoch 18 Batch 20 Loss 3.7678
    stored!
    val Epoch 18 avaLoss 3.7238
    Time taken for 1 epoch 0.07560661329163446h
    
    train Epoch 19 Batch 0 Loss 3.3377
    train Epoch 19 Batch 100 Loss 3.4643
    train Epoch 19 avaLoss 3.4291
    val Epoch 19 Batch 0 Loss 3.7335
    val Epoch 19 Batch 20 Loss 3.7152
    stored!
    val Epoch 19 avaLoss 3.7122
    Time taken for 1 epoch 0.07556267023086548h
    
    train Epoch 20 Batch 0 Loss 3.3496
    train Epoch 20 Batch 100 Loss 3.4060
    train Epoch 20 avaLoss 3.3902
    val Epoch 20 Batch 0 Loss 3.6610
    val Epoch 20 Batch 20 Loss 3.7108
    stored!
    val Epoch 20 avaLoss 3.6999
    Time taken for 1 epoch 0.07546679476896921h
    
    train Epoch 21 Batch 0 Loss 3.3116
    train Epoch 21 Batch 100 Loss 3.3273
    train Epoch 21 avaLoss 3.3527
    val Epoch 21 Batch 0 Loss 3.6475
    val Epoch 21 Batch 20 Loss 3.7033
    stored!
    val Epoch 21 avaLoss 3.6941
    Time taken for 1 epoch 0.07540691634019216h
    
    train Epoch 22 Batch 0 Loss 3.3017
    train Epoch 22 Batch 100 Loss 3.3267
    train Epoch 22 avaLoss 3.3175
    val Epoch 22 Batch 0 Loss 3.7996
    val Epoch 22 Batch 20 Loss 3.7448
    stored!
    val Epoch 22 avaLoss 3.6888
    Time taken for 1 epoch 0.07558426651689741h
    
    train Epoch 23 Batch 0 Loss 3.2593
    train Epoch 23 Batch 100 Loss 3.2138
    train Epoch 23 avaLoss 3.2831
    val Epoch 23 Batch 0 Loss 3.6571
    val Epoch 23 Batch 20 Loss 3.7266
    stored!
    val Epoch 23 avaLoss 3.6846
    Time taken for 1 epoch 0.07561517728699578h
    
    train Epoch 24 Batch 0 Loss 3.1550
    train Epoch 24 Batch 100 Loss 3.2604
    train Epoch 24 avaLoss 3.2506
    val Epoch 24 Batch 0 Loss 3.5455
    val Epoch 24 Batch 20 Loss 3.7293
    stored!
    val Epoch 24 avaLoss 3.6827
    Time taken for 1 epoch 0.07553693407111697h
    
    train Epoch 25 Batch 0 Loss 3.2102
    train Epoch 25 Batch 100 Loss 3.2171
    train Epoch 25 avaLoss 3.2193
    val Epoch 25 Batch 0 Loss 3.6639
    val Epoch 25 Batch 20 Loss 3.6237
    stored!
    val Epoch 25 avaLoss 3.6809
    Time taken for 1 epoch 0.07565948181682162h
    
    train Epoch 26 Batch 0 Loss 3.1642
    train Epoch 26 Batch 100 Loss 3.3191
    train Epoch 26 avaLoss 3.1874
    val Epoch 26 Batch 0 Loss 3.6038
    val Epoch 26 Batch 20 Loss 3.6036
    val Epoch 26 avaLoss 3.6825
    Time taken for 1 epoch 0.07512907935513391h
    
    train Epoch 27 Batch 0 Loss 3.1835
    train Epoch 27 Batch 100 Loss 3.2040
    train Epoch 27 avaLoss 3.1578
    val Epoch 27 Batch 0 Loss 3.7427
    val Epoch 27 Batch 20 Loss 3.6593
    val Epoch 27 avaLoss 3.6846
    Time taken for 1 epoch 0.0747886198759079h
    
    train Epoch 28 Batch 0 Loss 3.0469
    train Epoch 28 Batch 100 Loss 3.1888
    train Epoch 28 avaLoss 3.1287
    val Epoch 28 Batch 0 Loss 3.6172
    val Epoch 28 Batch 20 Loss 3.6387
    val Epoch 28 avaLoss 3.6887
    Time taken for 1 epoch 0.07512899955113729h
    
    train Epoch 29 Batch 0 Loss 3.1038
    train Epoch 29 Batch 100 Loss 3.0259
    train Epoch 29 avaLoss 3.0992
    val Epoch 29 Batch 0 Loss 3.5858
    val Epoch 29 Batch 20 Loss 3.7711
    val Epoch 29 avaLoss 3.6934
    Time taken for 1 epoch 0.07511097974247402h
    
    train Epoch 30 Batch 0 Loss 3.0265
    train Epoch 30 Batch 100 Loss 3.0594
    train Epoch 30 avaLoss 3.0746
    val Epoch 30 Batch 0 Loss 3.7100
    val Epoch 30 Batch 20 Loss 3.6663
    val Epoch 30 avaLoss 3.7019
    Time taken for 1 epoch 0.07525251150131225h
    
    5è½®å‚æ•°æ²¡æœ‰ä¼˜åŒ–ï¼Œæ—©åœ





    0




```python
import matplotlib.pyplot as plt  
def draw_loss(a,b):
    x_list=[]
    for i in range(len(a)):
        x_list.append(i)
    plt.title("LOSS")
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.plot(x_list,a,marker='s',label="train")
    plt.plot(x_list,b,marker='s',label="val")
    plt.legend()
    plt.savefig('work/output/LOSS.png')
    # plt.imgshow()
draw_loss(train_loss_list,val_loss_list)
```


    
![png](output_33_0.png)
    


## äº” æ¨¡å‹è¯„ä¼°
åŒ…æ‹¬lossç­‰
æœ€åè¾“å‡ºç»“æœ
æ ·ä¾‹æ•°æ®ç­‰

### 5.1 lossæŸ¥çœ‹


```python
print(val_loss_list)
```

    [5.107610457982773, 4.9817137106870995, 4.820859077649239, 4.675857935196314, 4.524792010967548, 4.392579298753005, 4.273853791065705, 4.172863300030048, 4.0873248760516825, 4.01116943359375, 3.945782783703926, 3.893668150290465, 3.849043234800681, 3.8130555764222755, 3.782649113581731, 3.757319914988982, 3.7402887588892226, 3.7238264817457933, 3.7121507693559694, 3.699888962965745, 3.6940949268830128, 3.688780955779247, 3.6845636612329726, 3.682664528871194, 3.680921114408053, 3.6824806409004407, 3.684640346429287, 3.688658494215745, 3.693355462489984, 3.7019277719350963]



```python
import matplotlib.pyplot as plt  
def draw_loss(a,b):
    x_list=[]
    for i in range(len(a)):
        x_list.append(i)
    plt.title("LOSS")
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.plot(x_list,a,marker='s',label="train")
    plt.plot(x_list,b,marker='s',label="val")
    plt.legend()
    plt.savefig('work/output/LOSS.png')
    # plt.imgshow()
draw_loss(train_loss_list,val_loss_list)
```


    
![png](output_37_0.png)
    


### 5.2 valæŒ‡æ ‡


```python
#è¯„ä¼°æ—¶ï¼Œè¦å¤„äºæ¨ç†é˜¶æ®µ
encoder.eval()
decoder.eval()
val_total_loss=0
for (batch, (inp,_, targ,_)) in enumerate(val_loader):
    #print(batch,inp.shape,targ.shape)
    batch_loss = val_step(inp, targ)
    val_total_loss += batch_loss
    if batch % 20 == 0:
        print('val Batch {} Loss {:.4f}'.format(batch,batch_loss.numpy()[0]))
val_loss_now=val_total_loss.numpy()[0] / val_batch_num
print(val_loss_now)
```

    val Batch 0 Loss 3.6383
    val Batch 20 Loss 3.6728
    3.7015099158653846


### 5.3 testæŒ‡æ ‡


```python
encoder.eval()
decoder.eval()
test_total_loss=0
for (batch, (inp,_, targ,_)) in enumerate(test_loader):
    #print(batch,inp.shape,targ.shape)
    batch_loss = val_step(inp, targ)
    test_total_loss += batch_loss
    if batch % 20 == 0:
        print('test Batch {} Loss {:.4f}'.format(batch,batch_loss.numpy()[0]))
test_total_loss=test_total_loss.numpy()[0] / test_batch_num
print(test_total_loss)
```

    test Batch 0 Loss 3.5377
    3.7292022705078125


### 5.4 æŸ¥çœ‹ç»“æœ


```python
def evalute(code):
        result=''
        # code.shape(1,500)
        print(code.shape)
        code=paddle.unsqueeze(code,axis=0)
        code_encoding_out,code_encoding_hidden=encoder(code)
        hidden=code_encoding_hidden
        # decoder_input.shape(1,1)
        decoder_input=paddle.unsqueeze(paddle.to_tensor([word2id_dict['<start>']]),axis=0)
        for i in range(50):
            #(batch_size,output_target_len,target_vocab_size)
            pre,hidden=decoder(decoder_input,hidden,code_encoding_out)
            # å–é¢„æµ‹ç»“æœä¸­æ¦‚ç‡æœ€å¤§çš„å€¼
            pred_id=paddle.argmax(pre[0]).numpy()
            if id2word_dict.get(pred_id[0])=='<unk>':
                return result
            result+=id2word_dict.get(pred_id[0])+''
            # fed back
            decoder_input=paddle.unsqueeze(paddle.to_tensor(pred_id),0)
        return result
def gety(ys,yl):
    res=""
    ys=ys.numpy()
    for i in range(yl):
        res+=id2word_dict.get(ys[i],'N')+''
    return res
```


```python
example_x,xl, example_y ,yl = next(iter(test_loader))
```


```python
index=0
print("=====æ¨¡å‹è¾“å‡ºçš„ç»“æœ====")
print(evalute(example_x[index]))
print("=====çœŸå®çš„ç»“æœ====")
print(gety(example_y[index],yl[index]))
```

    =====æ¨¡å‹è¾“å‡ºçš„ç»“æœ====
    [240]
    æ¬¾è¡¬è¡«é‡‡ç”¨äº†ä¼˜è´¨é¢æ–™æ‰“é€ è€Œæˆè´¨åœ°æŸ”è½¯èˆ’é€‚é€æ°”ç©¿ç€èˆ’é€‚ä¸ç´§ç»·æ„Ÿåè¶³çš„åŒæ—¶ä¹Ÿèƒ½ä¿®é¥°è…¿å‹çš„ç‰ˆå‹è®¾è®¡ä¿®é¥°é¢ˆéƒ¨çº¿æ¡å‡¸æ˜¾å¥³æ€§çš„æ·‹æ¼“å°½è‡´ï¼›é‡‡ç”¨äº†ä¼˜è´¨é¢æ–™æ‰“é€ è€Œæˆè´¨åœ°æŸ”è½¯èˆ’é€‚é€æ°”ç©¿ç€èˆ’é€‚ä¸ç´§ç»·
    =====çœŸå®çš„ç»“æœ====
    é€‰ç”¨å¸æ¹¿é€æ°”çš„ç‰›ä»”é¢æ–™åˆ¶æˆæ‰‹æ„ŸæŸ”è½¯ç©¿ç€èˆ’é€‚è‡ªåœ¨æ–¯æ–‡é›…è‡´ç¿»é¢†è®¾è®¡èåˆç²¾æ¹›æ´—æ°´å·¥è‰ºæ‰“é€ å±‚æ¬¡ä¸°å¯Œçš„è‡ªç„¶å¤ç€æ„Ÿä¼‘é—²ç™¾æ­å£è¢‹çš„è£…é¥°æå‡é€ å‹åŒæ—¶åˆå¾ˆå®ç”¨




## å…¶ä»–éƒ¨åˆ†

### aistudioé“¾æ¥ (https://aistudio.baidu.com/aistudio/projectdetail/3571825)

### githubé“¾æ¥ (https://github.com/yuanyihan/aipp)

### giteeé“¾æ¥ (https://gitee.com/yuanyihan/aipp)

